#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•CandidateTaggerä¸­çš„LLMClienté—®é¢˜
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from api.llm import LLMClient
from function.candidate_tagger import CandidateTagger

def test_direct_llm_client():
    """ç›´æ¥æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    print("ğŸ§ª ç›´æ¥æµ‹è¯•LLMå®¢æˆ·ç«¯...")
    
    llm = LLMClient()
    
    test_messages = [{"role": "user", "content": "Say hello"}]
    
    # æµ‹è¯•Gemini
    response = llm.call_llm(
        provider="gemini",
        model="gemini-2.5-flash-lite", 
        messages=test_messages,
        temperature=0.1,
        max_tokens=10
    )
    
    print(f"ç›´æ¥è°ƒç”¨ç»“æœ: success={response.get('success')}")
    if response.get('success'):
        print(f"å“åº”: {llm.get_response_content(response)}")
    else:
        print(f"é”™è¯¯: {response.get('error')}")

def test_candidate_tagger_llm():
    """æµ‹è¯•CandidateTaggerä¸­çš„LLMå®¢æˆ·ç«¯"""
    print("\nğŸ§ª æµ‹è¯•CandidateTaggerä¸­çš„LLMå®¢æˆ·ç«¯...")
    
    tagger = CandidateTagger(
        model="gemini-2.5-flash-lite",
        provider="gemini",
        temperature=0.1,
        max_tokens=10
    )
    
    test_messages = [{"role": "user", "content": "Say hello"}]
    
    # ç›´æ¥æµ‹è¯•taggerå†…éƒ¨çš„LLMå®¢æˆ·ç«¯
    response = tagger.llm_client.call_llm(
        provider="gemini",
        model="gemini-2.5-flash-lite",
        messages=test_messages,
        temperature=0.1,
        max_tokens=10
    )
    
    print(f"Taggerå†…éƒ¨è°ƒç”¨ç»“æœ: success={response.get('success')}")
    if response.get('success'):
        print(f"å“åº”: {tagger.llm_client.get_response_content(response)}")
    else:
        print(f"é”™è¯¯: {response.get('error')}")

def test_candidate_tagger_full():
    """æµ‹è¯•CandidateTaggerå®Œæ•´æµç¨‹"""
    print("\nğŸ§ª æµ‹è¯•CandidateTaggerå®Œæ•´æµç¨‹...")
    
    tagger = CandidateTagger(
        model="gemini-2.5-flash-lite",
        provider="gemini",
        temperature=0.1,
        max_tokens=500
    )
    
    # æµ‹è¯•å®é™…çš„å€™é€‰äººæ ‡ç­¾åŠŸèƒ½
    test_text = "We are looking for a backend engineer for our financial department. We hope this candidate have more than 5 years"
    
    result = tagger.analyze_text(test_text)
    
    print(f"å®Œæ•´æµç¨‹ç»“æœ: success={result.get('success')}")
    if result.get('success'):
        print(f"è§£æç»“æœ: {result.get('result')}")
        print(f"åŸå§‹å“åº”é•¿åº¦: {len(result.get('raw_content', ''))}")
    else:
        print(f"é”™è¯¯: {result.get('error')}")
        if 'raw_content' in result:
            print(f"åŸå§‹å†…å®¹: {result['raw_content']}")

def test_with_debug_params():
    """æµ‹è¯•å‚æ•°ä¼ é€’"""
    print("\nğŸ§ª æµ‹è¯•å‚æ•°ä¼ é€’...")
    
    tagger = CandidateTagger(
        model="gemini-2.5-flash-lite", 
        provider="gemini",
        temperature=0.1,
        max_tokens=500,
        frequency_penalty=0.0,
        presence_penalty=0.0
    )
    
    print(f"Taggerå‚æ•°:")
    print(f"  temperature: {tagger.temperature}")
    print(f"  frequency_penalty: {tagger.frequency_penalty}")
    print(f"  presence_penalty: {tagger.presence_penalty}")
    
    # æµ‹è¯•ç®€å•è°ƒç”¨
    test_text = "Backend engineer needed"
    result = tagger.analyze_text(test_text)
    
    print(f"å‚æ•°æµ‹è¯•ç»“æœ: success={result.get('success')}")
    if not result.get('success'):
        print(f"é”™è¯¯: {result.get('error')}")

if __name__ == "__main__":
    print("ğŸ” å¼€å§‹è¯Šæ–­CandidateTaggerå’ŒLLMClientçš„é›†æˆé—®é¢˜...")
    print("=" * 60)
    
    test_direct_llm_client()
    test_candidate_tagger_llm() 
    test_candidate_tagger_full()
    test_with_debug_params()
    
    print("\nğŸ¯ è¯Šæ–­å®Œæˆï¼")