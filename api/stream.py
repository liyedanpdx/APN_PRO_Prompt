"""
ç»Ÿä¸€æµå¼å“åº”å°è£…æ¨¡å—
æ”¯æŒå¤šç§ LLM æä¾›å•†çš„æµå¼è¾“å‡ºç»Ÿä¸€å¤„ç†ï¼Œæ–¹ä¾¿å‰ç«¯é›†æˆ
"""

import json
from typing import Dict, Any, Optional, Generator, Iterator
from enum import Enum
import time


class StreamType(Enum):
    """æµå¼å“åº”ç±»å‹æšä¸¾"""
    OPENAI_LIKE = "openai_like"  # OpenAI, Gemini, Perplexity
    REQUESTS = "requests"        # Groq, Ali (SSE format)


class ChunkData:
    """æ ‡å‡†åŒ–çš„æµå¼æ•°æ®å—"""
    
    def __init__(
        self,
        content: Optional[str] = None,
        chunk_type: str = "content",
        finish_reason: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None
    ):
        self.content = content or ""
        self.type = chunk_type
        self.finish_reason = finish_reason
        self.metadata = metadata or {}
        self.error = error
        self.timestamp = time.time()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "content": self.content,
            "type": self.type,
            "finish_reason": self.finish_reason,
            "metadata": self.metadata,
            "error": self.error,
            "timestamp": self.timestamp
        }
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        return json.dumps(self.to_dict(), ensure_ascii=False)


class UniversalStream:
    """ç»Ÿä¸€æµå¼å¤„ç†å™¨"""
    
    def __init__(
        self, 
        provider_stream, 
        stream_type: StreamType,
        provider_name: str = "unknown"
    ):
        self.provider_stream = provider_stream
        self.stream_type = stream_type
        self.provider_name = provider_name
        self.finished = False
        self.total_content = ""
        self.chunk_count = 0
        
    def __iter__(self):
        return self
    
    def __next__(self) -> ChunkData:
        """ç»Ÿä¸€çš„æµå¼æ•°æ®è¿­ä»£"""
        if self.finished:
            raise StopIteration
            
        try:
            if self.stream_type == StreamType.OPENAI_LIKE:
                chunk_data = self._handle_openai_chunk()
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if chunk_data.finish_reason is not None:
                    self.finished = True
                return chunk_data
            elif self.stream_type == StreamType.REQUESTS:
                return self._handle_requests_chunk()
            else:
                raise ValueError(f"Unsupported stream type: {self.stream_type}")
                
        except StopIteration:
            self.finished = True
            raise
        except Exception as e:
            self.finished = True
            error_chunk = ChunkData(
                chunk_type="error",
                error=str(e),
                metadata={"provider": self.provider_name}
            )
            return error_chunk
    
    def _handle_openai_chunk(self) -> ChunkData:
        """å¤„ç† OpenAI é£æ ¼çš„æµå¼æ•°æ®"""
        chunk = next(self.provider_stream)
        
        content = ""
        finish_reason = None
        metadata = {}
        
        # æå–å†…å®¹
        if hasattr(chunk, 'choices') and chunk.choices:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                self.total_content += content
                self.chunk_count += 1
            
            finish_reason = chunk.choices[0].finish_reason
        
        # æå–å…ƒæ•°æ®ï¼ˆPerplexity ç‰¹æœ‰ï¼‰
        if hasattr(chunk, 'usage') and chunk.usage:
            metadata['usage'] = {
                'prompt_tokens': getattr(chunk.usage, 'prompt_tokens', None),
                'completion_tokens': getattr(chunk.usage, 'completion_tokens', None),
                'total_tokens': getattr(chunk.usage, 'total_tokens', None)
            }
            
        if hasattr(chunk, 'search_results') and chunk.search_results:
            metadata['search_results'] = chunk.search_results
        
        return ChunkData(
            content=content,
            finish_reason=finish_reason,
            metadata=metadata
        )
    
    def _handle_requests_chunk(self) -> ChunkData:
        """å¤„ç† requests SSE é£æ ¼çš„æµå¼æ•°æ®"""
        for line in self.provider_stream.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data_str = line[6:]  # å»æ‰ 'data: ' å‰ç¼€
                    if data_str.strip() == '[DONE]':
                        raise StopIteration
                    
                    try:
                        data = json.loads(data_str)
                        if 'choices' in data and data['choices']:
                            delta = data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            finish_reason = data['choices'][0].get('finish_reason')
                            
                            if content:
                                self.total_content += content
                                self.chunk_count += 1
                                
                            return ChunkData(
                                content=content,
                                finish_reason=finish_reason
                            )
                    except json.JSONDecodeError:
                        continue
        
        raise StopIteration


class StreamResponse:
    """æµå¼å“åº”å°è£…å™¨"""
    
    def __init__(self, universal_stream: UniversalStream):
        self.stream = universal_stream
        
    def to_sse(self) -> Generator[str, None, None]:
        """è½¬æ¢ä¸º Server-Sent Events æ ¼å¼"""
        try:
            for chunk in self.stream:
                # å‘é€æœ‰å†…å®¹çš„å—
                if chunk.content or chunk.error:
                    yield f"data: {chunk.to_json()}\n\n"
                
                # å¦‚æœé‡åˆ°ç»“æŸæ ‡å¿—ï¼Œå‘é€æœ€ç»ˆå—å¹¶ç»“æŸ
                if chunk.finish_reason is not None:
                    final_chunk = ChunkData(
                        chunk_type="finish",
                        finish_reason=chunk.finish_reason,
                        metadata={
                            "total_chunks": getattr(self.stream, 'chunk_count', 0),
                            "total_content_length": len(getattr(self.stream, 'total_content', '')),
                            "provider": getattr(self.stream, 'provider_name', 'unknown')
                        }
                    )
                    yield f"data: {final_chunk.to_json()}\n\n"
                    break
            
        except StopIteration:
            pass
        finally:
            # å§‹ç»ˆå‘é€ç»“æŸæ ‡å¿—
            yield "data: [DONE]\n\n"
    
    def to_websocket(self) -> Generator[str, None, None]:
        """è½¬æ¢ä¸º WebSocket æ ¼å¼"""
        try:
            for chunk in self.stream:
                if chunk.content or chunk.error or chunk.finish_reason:
                    yield chunk.to_json()
                    
        except StopIteration as e:
            if hasattr(e, 'value') and e.value:
                yield e.value.to_json()
    
    def collect_full_response(self) -> Dict[str, Any]:
        """æ”¶é›†å®Œæ•´å“åº”ï¼ˆéæµå¼æ¨¡å¼ï¼‰"""
        full_content = ""
        final_metadata = {}
        chunk_count = 0
        
        try:
            for chunk in self.stream:
                if chunk.content:
                    full_content += chunk.content
                    chunk_count += 1
                if chunk.metadata:
                    final_metadata.update(chunk.metadata)
                if chunk.error:
                    return {
                        "success": False,
                        "error": chunk.error,
                        "content": full_content,
                        "metadata": final_metadata
                    }
                    
        except StopIteration as e:
            if hasattr(e, 'value') and e.value:
                final_metadata.update(e.value.metadata)
        
        return {
            "success": True,
            "content": full_content,
            "metadata": final_metadata,
            "chunk_count": chunk_count
        }


def create_stream_response(provider_stream, provider_name: str) -> StreamResponse:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºç»Ÿä¸€çš„æµå¼å“åº”
    
    Args:
        provider_stream: åŸå§‹æä¾›å•†æµå¯¹è±¡
        provider_name: æä¾›å•†åç§° ("openai", "gemini", "perplexity", "groq", "ali")
    
    Returns:
        StreamResponse: ç»Ÿä¸€çš„æµå¼å“åº”å¯¹è±¡
    """
    # æ ¹æ®æä¾›å•†åˆ¤æ–­æµç±»å‹
    if provider_name.lower() in ["openai", "gemini", "perplexity"]:
        stream_type = StreamType.OPENAI_LIKE
    else:
        stream_type = StreamType.REQUESTS
    
    universal_stream = UniversalStream(provider_stream, stream_type, provider_name)
    return StreamResponse(universal_stream)


# ç®€å•çš„ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸŒŠ æµå¼å“åº”å°è£…æ¨¡å—")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿåˆ›å»ºæµå¼å“åº”çš„ç¤ºä¾‹
    print("ğŸ“ ç¤ºä¾‹ç”¨æ³•ï¼š")
    print("""
    # åŸºç¡€ç”¨æ³•
    from api.llm import LLMClient
    from api.stream import create_stream_response
    
    llm = LLMClient()
    raw_stream = llm.stream_llm("openai", "gpt-4", messages)
    stream_response = create_stream_response(raw_stream, "openai")
    
    # SSE æ ¼å¼ (ç”¨äºå‰ç«¯)
    for sse_data in stream_response.to_sse():
        print(sse_data, end='')
    
    # æ”¶é›†å®Œæ•´å“åº”
    full_response = stream_response.collect_full_response()
    print(full_response['content'])
    
    # WebSocket æ ¼å¼
    for ws_data in stream_response.to_websocket():
        websocket.send(ws_data)
    """)
    
    print("\nâœ… æ¨¡å—åŠ è½½æˆåŠŸï¼")
    print("ğŸ¯ æ”¯æŒçš„åŠŸèƒ½ï¼š")
    print("  - ç»Ÿä¸€å¤šç§ LLM æä¾›å•†çš„æµå¼è¾“å‡º")
    print("  - SSE (Server-Sent Events) æ ¼å¼è½¬æ¢")
    print("  - WebSocket æ ¼å¼è½¬æ¢")
    print("  - å®Œæ•´å“åº”æ”¶é›†")
    print("  - é”™è¯¯å¤„ç†å’Œå…ƒæ•°æ®æå–")