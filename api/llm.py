import openai
import requests
import json
from typing import Dict, Any, Optional, List
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config import Config

class LLMClient:
    def __init__(self):
        self.config = Config()
        
    def call_openai(
        self,
        model: str = "gpt-4-1106-preview",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨OpenAI API
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "gpt-4-1106-preview", "gpt-3.5-turbo" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            frequency_penalty: é¢‘ç‡æƒ©ç½š (-2.0 to 2.0)
            presence_penalty: å­˜åœ¨æƒ©ç½š (-2.0 to 2.0)
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages or [],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stop=stop,
                **kwargs
            )
            
            return {
                "success": True,
                "data": {
                    "choices": [
                        {
                            "message": {
                                "role": choice.message.role,
                                "content": choice.message.content
                            },
                            "finish_reason": choice.finish_reason
                        } for choice in response.choices
                    ],
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "model": response.model
                },
                "provider": "openai"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "openai"
            }
    
    def call_perplexity(
        self,
        model: str = "llama-3.1-sonar-small-128k-online",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 0.9,
        top_k: int = 0,
        stream: bool = False,
        presence_penalty: float = 0,
        frequency_penalty: float = 1,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨Perplexity API
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "llama-3.1-sonar-small-128k-online", "llama-3.1-sonar-large-128k-online" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            top_k: top-ké‡‡æ ·å‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            presence_penalty: å­˜åœ¨æƒ©ç½š
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = "https://api.perplexity.ai/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "stream": stream,
            "presence_penalty": presence_penalty,
            "frequency_penalty": frequency_penalty,
            **kwargs
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "perplexity"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "perplexity"
            }
    
    def call_groq(
        self,
        model: str = "llama-3.1-70b-versatile",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨Groq API
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            stream: æ˜¯å¦æµå¼è¾“å‡º
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.GROQ_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream,
            **kwargs
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "groq"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "groq"
            }
    
    def call_ali(
        self,
        model: str = "deepseek-v3",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨é˜¿é‡Œäº‘API (é€šè¿‡DashScopeå…¼å®¹OpenAIæ ¼å¼)
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "deepseek-v3", "qwen-max", "qwen-turbo" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            stream: æ˜¯å¦æµå¼è¾“å‡º
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = f"{self.config.ALI_API_BASE}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.ALI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream,
            **kwargs
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "ali"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "ali"
            }
    
    def call_gemini(
        self,
        model: str = "gemini-2.5-flash-lite",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨Gemini API (é€šè¿‡OpenAIå…¼å®¹æ ¼å¼)
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "gemini-2.5-flash-lite", "gemini-1.5-pro" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            stream: æ˜¯å¦æµå¼è¾“å‡º
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = f"{self.config.GEMINI_API_BASE}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.GEMINI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # Geminiæ”¯æŒçš„å‚æ•°ï¼ˆè¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°ï¼‰
        supported_params = {}
        for key, value in kwargs.items():
            if key not in ['frequency_penalty', 'presence_penalty']:
                supported_params[key] = value
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream,
            **supported_params
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "gemini"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "gemini"
            }
    
    def call_llm(
        self,
        provider: str,
        model: str,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€LLMè°ƒç”¨æ¥å£
        
        Args:
            provider: æä¾›å•† ("openai", "perplexity", "groq", "ali", "gemini")
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        if provider.lower() == "openai":
            return self.call_openai(model=model, messages=messages, **kwargs)
        elif provider.lower() == "perplexity":
            return self.call_perplexity(model=model, messages=messages, **kwargs)
        elif provider.lower() == "groq":
            return self.call_groq(model=model, messages=messages, **kwargs)
        elif provider.lower() == "ali":
            return self.call_ali(model=model, messages=messages, **kwargs)
        elif provider.lower() == "gemini":
            return self.call_gemini(model=model, messages=messages, **kwargs)
        else:
            return {
                "success": False,
                "error": f"Unsupported provider: {provider}",
                "provider": provider
            }
    
    def get_response_content(self, response: Dict[str, Any]) -> Optional[str]:
        """
        ä»å“åº”ä¸­æå–å†…å®¹æ–‡æœ¬
        
        Args:
            response: LLM APIå“åº”
            
        Returns:
            å“åº”å†…å®¹æ–‡æœ¬
        """
        if not response.get("success"):
            return None
            
        data = response.get("data", {})
        
        if response.get("provider") == "openai":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "perplexity":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "groq":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "ali":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "gemini":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        
        return None


# é¢„è®¾æ¨¡å‹é…ç½®
OPENAI_MODELS = {
    "gpt-4-turbo": "gpt-4-1106-preview",
    "gpt-4": "gpt-4",
    "gpt-3.5-turbo": "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k": "gpt-3.5-turbo-16k"
}

PERPLEXITY_MODELS = {
    "sonar-small": "llama-3.1-sonar-small-128k-online",
    "sonar-large": "llama-3.1-sonar-large-128k-online",
    "sonar-huge": "llama-3.1-sonar-huge-128k-online"
}

GROQ_MODELS = {
    "llama-3.1-70b": "llama-3.1-70b-versatile",
    "llama-3.1-8b": "llama-3.1-8b-instant",
    "llama-3.2-1b": "llama-3.2-1b-preview",
    "llama-3.2-3b": "llama-3.2-3b-preview",
    "mixtral-8x7b": "mixtral-8x7b-32768",
    "gemma-7b": "gemma-7b-it",
    "gemma2-9b": "gemma2-9b-it"
}

ALI_MODELS = {
    "deepseek-v3": "deepseek-v3",
    "qwen-max": "qwen-max",
    "qwen-turbo": "qwen-turbo", 
    "qwen-plus": "qwen-plus",
    "qwen-long": "qwen-long",
    "qwen2.5-72b": "qwen2.5-72b-instruct",
    "qwen2.5-32b": "qwen2.5-32b-instruct",
    "qwen2.5-14b": "qwen2.5-14b-instruct",
    "qwen2.5-7b": "qwen2.5-7b-instruct"
}

GEMINI_MODELS = {
    "gemini-2.5-flash-lite": "gemini-2.5-flash-lite",
    "gemini-1.5-pro": "gemini-1.5-pro",
    "gemini-1.5-flash": "gemini-1.5-flash",
    "gemini-1.0-pro": "gemini-1.0-pro"
}

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ‰€æœ‰LLMæä¾›å•†...")
    print("=" * 60)
    
    llm = LLMClient()
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {"role": "user", "content": "who are you?"}
    ]
    
    # 1. OpenAIæµ‹è¯•
    print("\n1ï¸âƒ£ æµ‹è¯• OpenAI...")
    try:
        openai_response = llm.call_openai(
            model="gpt-4-1106-preview",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if openai_response["success"]:
            print("âœ… OpenAI è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(openai_response)}")
            if "usage" in openai_response.get("data", {}):
                usage = openai_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ OpenAI è°ƒç”¨å¤±è´¥: {openai_response['error']}")
    except Exception as e:
        print(f"âŒ OpenAI å¼‚å¸¸: {e}")
    
    # 2. Perplexityæµ‹è¯•  
    print("\n2ï¸âƒ£ æµ‹è¯• Perplexity...")
    try:
        perplexity_response = llm.call_perplexity(
            model="llama-3.1-sonar-small-128k-online",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if perplexity_response["success"]:
            print("âœ… Perplexity è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(perplexity_response)}")
            if "usage" in perplexity_response.get("data", {}):
                usage = perplexity_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Perplexity è°ƒç”¨å¤±è´¥: {perplexity_response['error']}")
    except Exception as e:
        print(f"âŒ Perplexity å¼‚å¸¸: {e}")
    
    # 3. Groqæµ‹è¯•
    print("\n3ï¸âƒ£ æµ‹è¯• Groq...")
    try:
        groq_response = llm.call_groq(
            model="llama-3.1-70b-versatile",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if groq_response["success"]:
            print("âœ… Groq è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(groq_response)}")
            if "usage" in groq_response.get("data", {}):
                usage = groq_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Groq è°ƒç”¨å¤±è´¥: {groq_response['error']}")
    except Exception as e:
        print(f"âŒ Groq å¼‚å¸¸: {e}")
    
    # 4. Ali (é˜¿é‡Œäº‘) æµ‹è¯•
    print("\n4ï¸âƒ£ æµ‹è¯• Ali (é˜¿é‡Œäº‘)...")
    try:
        ali_response = llm.call_ali(
            model="deepseek-v3",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if ali_response["success"]:
            print("âœ… Ali è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(ali_response)}")
            if "usage" in ali_response.get("data", {}):
                usage = ali_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Ali è°ƒç”¨å¤±è´¥: {ali_response['error']}")
    except Exception as e:
        print(f"âŒ Ali å¼‚å¸¸: {e}")
    
    # 5. Geminiæµ‹è¯•
    print("\n5ï¸âƒ£ æµ‹è¯• Gemini...")
    try:
        gemini_response = llm.call_gemini(
            model="gemini-2.5-flash-lite",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if gemini_response["success"]:
            print("âœ… Gemini è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(gemini_response)}")
            if "usage" in gemini_response.get("data", {}):
                usage = gemini_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Gemini è°ƒç”¨å¤±è´¥: {gemini_response['error']}")
    except Exception as e:
        print(f"âŒ Gemini å¼‚å¸¸: {e}")
    
    # 6. ç»Ÿä¸€æ¥å£æµ‹è¯•
    print("\n6ï¸âƒ£ æµ‹è¯•ç»Ÿä¸€æ¥å£ call_llm()...")
    providers_to_test = [
        ("openai", "gpt-4-1106-preview"),
        ("perplexity", "llama-3.1-sonar-small-128k-online"), 
        ("groq", "llama-3.1-70b-versatile"),
        ("ali", "deepseek-v3"),
        ("gemini", "gemini-2.5-flash-lite")
    ]
    
    for provider, model in providers_to_test:
        try:
            print(f"\nğŸ”„ æµ‹è¯•ç»Ÿä¸€æ¥å£: {provider} - {model}")
            response = llm.call_llm(
                provider=provider,
                model=model,
                messages=test_messages,
                temperature=0.1,
                max_tokens=10
            )
            
            if response["success"]:
                print(f"âœ… {provider} ç»Ÿä¸€æ¥å£è°ƒç”¨æˆåŠŸ")
                content = llm.get_response_content(response)
                print(f"ğŸ“ å“åº”: {content}")
            else:
                print(f"âŒ {provider} ç»Ÿä¸€æ¥å£è°ƒç”¨å¤±è´¥: {response['error']}")
        except Exception as e:
            print(f"âŒ {provider} ç»Ÿä¸€æ¥å£å¼‚å¸¸: {e}")
    
    print(f"\n{'=' * 60}")
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼è¯·æ£€æŸ¥ä¸Šè¿°ç»“æœä»¥ç¡®è®¤å„æä¾›å•†çš„å·¥ä½œçŠ¶æ€ã€‚")