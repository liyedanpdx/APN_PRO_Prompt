import openai
import requests
import json
from typing import Dict, Any, Optional, List, Generator, Callable
import sys
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config import Config

class LLMClient:
    def __init__(self):
        self.config = Config()
        
    def call_openai(
        self,
        model: str = "gpt-4-1106-preview",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨OpenAI API
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "gpt-4-1106-preview", "gpt-3.5-turbo" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            frequency_penalty: é¢‘ç‡æƒ©ç½š (-2.0 to 2.0)
            presence_penalty: å­˜åœ¨æƒ©ç½š (-2.0 to 2.0)
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages or [],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stop=stop,
                **kwargs
            )
            
            return {
                "success": True,
                "data": {
                    "choices": [
                        {
                            "message": {
                                "role": choice.message.role,
                                "content": choice.message.content
                            },
                            "finish_reason": choice.finish_reason
                        } for choice in response.choices
                    ],
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "model": response.model
                },
                "provider": "openai"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "openai"
            }
    
    def call_perplexity(
        self,
        model: str = "llama-3.1-sonar-small-128k-online",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 0.9,
        top_k: int = 0,
        stream: bool = False,
        presence_penalty: float = 0,
        frequency_penalty: float = 1,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨Perplexity API - ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "sonar-pro", "llama-3.1-sonar-small-128k-online" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            top_k: top-ké‡‡æ ·å‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            presence_penalty: å­˜åœ¨æƒ©ç½š
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ï¼Œä½†æŒ‡å‘ Perplexity çš„ç«¯ç‚¹
        client = openai.OpenAI(
            api_key=self.config.PERPLEXITY_API_KEY,
            base_url="https://api.perplexity.ai"
        )
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages or [],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                stream=stream,
                **kwargs
            )
            
            return {
                "success": True,
                "data": {
                    "choices": [
                        {
                            "message": {
                                "role": response.choices[0].message.role,
                                "content": response.choices[0].message.content
                            },
                            "finish_reason": response.choices[0].finish_reason
                        }
                    ],
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "model": response.model
                },
                "provider": "perplexity"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "perplexity"
            }
    
    def call_groq(
        self,
        model: str = "llama-3.1-70b-versatile",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨Groq API
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            stream: æ˜¯å¦æµå¼è¾“å‡º
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.GROQ_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream,
            **kwargs
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "groq"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "groq"
            }
    
    def call_ali(
        self,
        model: str = "deepseek-v3",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨é˜¿é‡Œäº‘API (é€šè¿‡DashScopeå…¼å®¹OpenAIæ ¼å¼)
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "deepseek-v3", "qwen-max", "qwen-turbo" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            stream: æ˜¯å¦æµå¼è¾“å‡º
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = f"{self.config.ALI_API_BASE}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.ALI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream,
            **kwargs
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "ali"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "ali"
            }
    
    def call_gemini(
        self,
        model: str = "gemini-2.5-flash-lite",
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        stream: bool = False,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è°ƒç”¨Gemini API (é€šè¿‡OpenAIå…¼å®¹æ ¼å¼)
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "gemini-2.5-flash-lite", "gemini-1.5-pro" ç­‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "text"}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§tokenæ•°
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)
            stream: æ˜¯å¦æµå¼è¾“å‡º
            stop: åœæ­¢åºåˆ—
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        url = f"{self.config.GEMINI_API_BASE}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.GEMINI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # Geminiæ”¯æŒçš„å‚æ•°ï¼ˆè¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°ï¼‰
        supported_params = {}
        for key, value in kwargs.items():
            if key not in ['frequency_penalty', 'presence_penalty']:
                supported_params[key] = value
        
        payload = {
            "model": model,
            "messages": messages or [],
            "temperature": temperature,
            "top_p": top_p,
            "stream": stream,
            **supported_params
        }
        
        if max_tokens:
            payload["max_tokens"] = max_tokens
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            return {
                "success": True,
                "data": data,
                "provider": "gemini"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "gemini"
            }
    
    def call_llm(
        self,
        provider: str,
        model: str,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€LLMè°ƒç”¨æ¥å£
        
        Args:
            provider: æä¾›å•† ("openai", "perplexity", "groq", "ali", "gemini")
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            APIå“åº”å­—å…¸
        """
        if provider.lower() == "openai":
            return self.call_openai(model=model, messages=messages, **kwargs)
        elif provider.lower() == "perplexity":
            return self.call_perplexity(model=model, messages=messages, **kwargs)
        elif provider.lower() == "groq":
            return self.call_groq(model=model, messages=messages, **kwargs)
        elif provider.lower() == "ali":
            return self.call_ali(model=model, messages=messages, **kwargs)
        elif provider.lower() == "gemini":
            return self.call_gemini(model=model, messages=messages, **kwargs)
        else:
            return {
                "success": False,
                "error": f"Unsupported provider: {provider}",
                "provider": provider
            }
    
    def get_response_content(self, response: Dict[str, Any]) -> Optional[str]:
        """
        ä»å“åº”ä¸­æå–å†…å®¹æ–‡æœ¬
        
        Args:
            response: LLM APIå“åº”
            
        Returns:
            å“åº”å†…å®¹æ–‡æœ¬
        """
        if not response.get("success"):
            return None
            
        data = response.get("data", {})
        
        if response.get("provider") == "openai":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "perplexity":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "groq":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "ali":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        elif response.get("provider") == "gemini":
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content")
        
        return None
    
    def stream_llm(
        self,
        provider: str,
        model: str,
        messages: List[Dict[str, str]],
        **kwargs
    ):
        """
        æµå¼è°ƒç”¨LLMæ¥å£ - ç®€å•ç›´æ¥è¿”å›æµå¯¹è±¡
        
        Args:
            provider: æä¾›å•† ("openai", "gemini", "ali", "groq")
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æµå¯¹è±¡ï¼Œå¯ä»¥ç›´æ¥è¿­ä»£ä½¿ç”¨
        """
        kwargs['stream'] = True
        
        if provider.lower() == "openai":
            return self._stream_openai(model, messages, **kwargs)
        elif provider.lower() == "gemini":
            return self._stream_gemini(model, messages, **kwargs)
        elif provider.lower() == "ali":
            return self._stream_ali(model, messages, **kwargs)
        elif provider.lower() == "groq":
            return self._stream_groq(model, messages, **kwargs)
        elif provider.lower() == "perplexity":
            return self._stream_perplexity(model, messages, **kwargs)
        else:
            raise ValueError(f"Unsupported provider for streaming: {provider}")
    
    def _stream_openai(self, model: str, messages: List[Dict[str, str]], **kwargs):
        """OpenAI æµå¼ç”Ÿæˆ - ä½¿ç”¨åŸç”Ÿ OpenAI API"""
        client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
        return client.chat.completions.create(
            model=model,
            messages=messages or [],
            **kwargs
        )
    
    def _stream_gemini(self, model: str, messages: List[Dict[str, str]], **kwargs):
        """Gemini æµå¼ç”Ÿæˆ - ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ç»Ÿä¸€æ¥å£"""
        # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ï¼Œä½†æŒ‡å‘ Gemini çš„ç«¯ç‚¹
        client = openai.OpenAI(
            api_key=self.config.GEMINI_API_KEY,
            base_url=self.config.GEMINI_API_BASE
        )
        
        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°
        supported_params = {}
        for key, value in kwargs.items():
            if key not in ['frequency_penalty', 'presence_penalty']:
                supported_params[key] = value
        
        return client.chat.completions.create(
            model=model,
            messages=messages or [],
            **supported_params
        )
    
    def _stream_ali(self, model: str, messages: List[Dict[str, str]], **kwargs):
        """é˜¿é‡Œäº‘ æµå¼ç”Ÿæˆ - è¿”å›requestsæµå¯¹è±¡"""
        url = f"{self.config.ALI_API_BASE}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.ALI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            **kwargs
        }
        
        response = requests.post(url, json=payload, headers=headers, stream=True)
        response.raise_for_status()
        return response
    
    def _stream_groq(self, model: str, messages: List[Dict[str, str]], **kwargs):
        """Groq æµå¼ç”Ÿæˆ - è¿”å›requestsæµå¯¹è±¡"""
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.GROQ_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages or [],
            **kwargs
        }
        
        response = requests.post(url, json=payload, headers=headers, stream=True)
        response.raise_for_status()
        return response
    
    def _stream_perplexity(self, model: str, messages: List[Dict[str, str]], **kwargs):
        """Perplexity æµå¼ç”Ÿæˆ - ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ç»Ÿä¸€æ¥å£"""
        # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ï¼Œä½†æŒ‡å‘ Perplexity çš„ç«¯ç‚¹
        client = openai.OpenAI(
            api_key=self.config.PERPLEXITY_API_KEY,
            base_url="https://api.perplexity.ai"
        )
        
        return client.chat.completions.create(
            model=model,
            messages=messages or [],
            **kwargs
        )


# é¢„è®¾æ¨¡å‹é…ç½®
OPENAI_MODELS = {
    "gpt-4-turbo": "gpt-4-1106-preview",
    "gpt-4": "gpt-4",
    "gpt-3.5-turbo": "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k": "gpt-3.5-turbo-16k"
}

PERPLEXITY_MODELS = {
    "sonar-small": "llama-3.1-sonar-small-128k-online",
    "sonar-large": "llama-3.1-sonar-large-128k-online",
    "sonar-huge": "llama-3.1-sonar-huge-128k-online"
}

GROQ_MODELS = {
    "llama-3.1-70b": "llama-3.1-70b-versatile",
    "llama-3.1-8b": "llama-3.1-8b-instant",
    "llama-3.2-1b": "llama-3.2-1b-preview",
    "llama-3.2-3b": "llama-3.2-3b-preview",
    "mixtral-8x7b": "mixtral-8x7b-32768",
    "gemma-7b": "gemma-7b-it",
    "gemma2-9b": "gemma2-9b-it"
}

ALI_MODELS = {
    "deepseek-v3": "deepseek-v3",
    "qwen-max": "qwen-max",
    "qwen-turbo": "qwen-turbo", 
    "qwen-plus": "qwen-plus",
    "qwen-long": "qwen-long",
    "qwen2.5-72b": "qwen2.5-72b-instruct",
    "qwen2.5-32b": "qwen2.5-32b-instruct",
    "qwen2.5-14b": "qwen2.5-14b-instruct",
    "qwen2.5-7b": "qwen2.5-7b-instruct"
}

GEMINI_MODELS = {
    "gemini-2.5-flash-lite": "gemini-2.5-flash-lite",
    "gemini-1.5-pro": "gemini-1.5-pro",
    "gemini-1.5-flash": "gemini-1.5-flash",
    "gemini-1.0-pro": "gemini-1.0-pro"
}

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ‰€æœ‰LLMæä¾›å•†...")
    print("=" * 60)
    
    llm = LLMClient()
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {"role": "user", "content": "who are you?"}
    ]
    
    # 1. OpenAIæµ‹è¯•
    print("\n1ï¸âƒ£ æµ‹è¯• OpenAI...")
    try:
        openai_response = llm.call_openai(
            model="gpt-4-1106-preview",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if openai_response["success"]:
            print("âœ… OpenAI è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(openai_response)}")
            if "usage" in openai_response.get("data", {}):
                usage = openai_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ OpenAI è°ƒç”¨å¤±è´¥: {openai_response['error']}")
    except Exception as e:
        print(f"âŒ OpenAI å¼‚å¸¸: {e}")
    
    # 2. Perplexityæµ‹è¯•  
    print("\n2ï¸âƒ£ æµ‹è¯• Perplexity...")
    try:
        perplexity_response = llm.call_perplexity(
            model="sonar",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if perplexity_response["success"]:
            print("âœ… Perplexity è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(perplexity_response)}")
            if "usage" in perplexity_response.get("data", {}):
                usage = perplexity_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Perplexity è°ƒç”¨å¤±è´¥: {perplexity_response['error']}")
    except Exception as e:
        print(f"âŒ Perplexity å¼‚å¸¸: {e}")
    
    # 3. Groqæµ‹è¯•
    print("\n3ï¸âƒ£ æµ‹è¯• Groq...")
    try:
        groq_response = llm.call_groq(
            model="llama-3.1-70b-versatile",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if groq_response["success"]:
            print("âœ… Groq è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(groq_response)}")
            if "usage" in groq_response.get("data", {}):
                usage = groq_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Groq è°ƒç”¨å¤±è´¥: {groq_response['error']}")
    except Exception as e:
        print(f"âŒ Groq å¼‚å¸¸: {e}")
    
    # 4. Ali (é˜¿é‡Œäº‘) æµ‹è¯•
    print("\n4ï¸âƒ£ æµ‹è¯• Ali (é˜¿é‡Œäº‘)...")
    try:
        ali_response = llm.call_ali(
            model="deepseek-v3",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10
        )
        
        if ali_response["success"]:
            print("âœ… Ali è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(ali_response)}")
            if "usage" in ali_response.get("data", {}):
                usage = ali_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Ali è°ƒç”¨å¤±è´¥: {ali_response['error']}")
    except Exception as e:
        print(f"âŒ Ali å¼‚å¸¸: {e}")
    
    # 5. Geminiæµ‹è¯•
    print("\n5ï¸âƒ£ æµ‹è¯• Gemini...")
    try:
        gemini_response = llm.call_gemini(
            model="gemini-2.5-flash-lite",
            messages=test_messages,
            temperature=0.1,
            max_tokens=10,
            stream=True
        )
        
        if gemini_response["success"]:
            print("âœ… Gemini è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ å“åº”: {llm.get_response_content(gemini_response)}")
            if "usage" in gemini_response.get("data", {}):
                usage = gemini_response["data"]["usage"]
                print(f"ğŸª™ Tokenä½¿ç”¨: {usage}")
        else:
            print(f"âŒ Gemini è°ƒç”¨å¤±è´¥: {gemini_response['error']}")
    except Exception as e:
        print(f"âŒ Gemini å¼‚å¸¸: {e}")
    
    # 6. ç»Ÿä¸€æ¥å£æµ‹è¯•
    print("\n6ï¸âƒ£ æµ‹è¯•ç»Ÿä¸€æ¥å£ call_llm()...")
    providers_to_test = [
        ("openai", "gpt-4-1106-preview"),
        ("perplexity", "llama-3.1-sonar-small-128k-online"), 
        ("groq", "llama-3.1-70b-versatile"),
        ("ali", "deepseek-v3"),
        ("gemini", "gemini-2.5-flash-lite")
    ]
    
    for provider, model in providers_to_test:
        try:
            print(f"\nğŸ”„ æµ‹è¯•ç»Ÿä¸€æ¥å£: {provider} - {model}")
            response = llm.call_llm(
                provider=provider,
                model=model,
                messages=test_messages,
                temperature=0.1,
                max_tokens=10
            )
            
            if response["success"]:
                print(f"âœ… {provider} ç»Ÿä¸€æ¥å£è°ƒç”¨æˆåŠŸ")
                content = llm.get_response_content(response)
                print(f"ğŸ“ å“åº”: {content}")
            else:
                print(f"âŒ {provider} ç»Ÿä¸€æ¥å£è°ƒç”¨å¤±è´¥: {response['error']}")
        except Exception as e:
            print(f"âŒ {provider} ç»Ÿä¸€æ¥å£å¼‚å¸¸: {e}")
    
    # 7. æµå¼ç”Ÿæˆæµ‹è¯•
    print("\n7ï¸âƒ£ æµ‹è¯•æµå¼ç”Ÿæˆ...")
    test_stream_providers = [
        ("openai", "gpt-4-1106-preview"),
        ("gemini", "gemini-2.5-flash-lite"),
        ("groq", "llama-3.1-70b-versatile"),
        ("ali", "deepseek-v3")
    ]
    
    stream_test_messages = [
        {"role": "user", "content": "è¯·ç”¨50ä¸ªå­—ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"}
    ]
    
    # æµ‹è¯• OpenAI æµå¼ç”Ÿæˆï¼ˆå‚è€ƒ stream_api_test.py çš„ç®€å•æ–¹å¼ï¼‰
    print(f"\nğŸŒŠ æµ‹è¯•æµå¼ç”Ÿæˆ: OpenAI")
    try:
        print(f"ğŸ“ æµå¼å“åº”: ", end="", flush=True)
        start_time = time.time()
        
        # è·å–æµå¯¹è±¡
        stream = llm.stream_llm(
            provider="openai",
            model="gpt-4-1106-preview",
            messages=stream_test_messages,
            temperature=0.3,
            max_tokens=100
        )
        
        # åƒ stream_api_test.py ä¸€æ ·ç®€å•å¤„ç†æµ
        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end='', flush=True)
                chunk_count += 1
        
        end_time = time.time()
        print()  # æ¢è¡Œ
        print(f"âœ… OpenAI æµå¼ç”ŸæˆæˆåŠŸ")
        print(f"ğŸ“Š ç»Ÿè®¡: {chunk_count} ä¸ªå—")
        print(f"â±ï¸ è€—æ—¶: {end_time - start_time:.2f}s")
        
    except Exception as e:
        print(f"âŒ OpenAI æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
    
    # æµ‹è¯• Gemini æµå¼ç”Ÿæˆï¼ˆç°åœ¨ä¹Ÿä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ç»Ÿä¸€æ¥å£ï¼‰
    print(f"\nğŸŒŠ æµ‹è¯•æµå¼ç”Ÿæˆ: Gemini")
    try:
        print(f"ğŸ“ æµå¼å“åº”: ", end="", flush=True)
        start_time = time.time()
        
        # è·å–æµå¯¹è±¡ - ç°åœ¨ Gemini ä¹Ÿè¿”å›å’Œ OpenAI ç›¸åŒçš„æµå¯¹è±¡
        stream = llm.stream_llm(
            provider="gemini",
            model="gemini-2.5-flash-lite",
            messages=stream_test_messages,
            temperature=0.3,
            max_tokens=100
        )
        
        # åƒ OpenAI ä¸€æ ·å¤„ç†æµ
        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end='', flush=True)
                chunk_count += 1
        
        end_time = time.time()
        print()  # æ¢è¡Œ
        print(f"âœ… Gemini æµå¼ç”ŸæˆæˆåŠŸ")
        print(f"ğŸ“Š ç»Ÿè®¡: {chunk_count} ä¸ªå—")
        print(f"â±ï¸ è€—æ—¶: {end_time - start_time:.2f}s")
        
    except Exception as e:
        print(f"âŒ Gemini æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
    
    # æµ‹è¯• Perplexity æµå¼ç”Ÿæˆï¼ˆä¹Ÿä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åº“ç»Ÿä¸€æ¥å£ï¼‰
    print(f"\nğŸŒŠ æµ‹è¯•æµå¼ç”Ÿæˆ: Perplexity")
    try:
        print(f"ğŸ“ æµå¼å“åº”: ", end="", flush=True)
        start_time = time.time()
        
        # è·å–æµå¯¹è±¡ - ç°åœ¨ Perplexity ä¹Ÿè¿”å›å’Œ OpenAI ç›¸åŒçš„æµå¯¹è±¡
        stream = llm.stream_llm(
            provider="perplexity",
            model="sonar-pro",#"sonar-reasoning",
            messages=[{"role": "user", "content": "Compare renewable energy technologies"}],
            temperature=0.3
        )
        
        # åƒ OpenAI ä¸€æ ·å¤„ç†æµï¼Œä½†åŒ…å« Perplexity çš„é¢å¤–ä¿¡æ¯å¤„ç†
        chunk_count = 0
        content = ""
        search_results = []
        usage_info = None
        
        for chunk in stream:
            # Content arrives progressively
            if chunk.choices[0].delta.content is not None:
                content_chunk = chunk.choices[0].delta.content
                content += content_chunk
                print(content_chunk, end="", flush=True)
                chunk_count += 1
            
            # Metadata arrives in final chunks (Perplexity specific)
            if hasattr(chunk, 'search_results') and chunk.search_results:
                search_results = chunk.search_results
            
            if hasattr(chunk, 'usage') and chunk.usage:
                usage_info = chunk.usage
            
            # Handle completion
            if chunk.choices[0].finish_reason is not None:
                print(f"\n\nFinish reason: {chunk.choices[0].finish_reason}")
                if search_results:
                    print(f"Search Results: {len(search_results)} results found")
                if usage_info:
                    print(f"Usage: {usage_info}")
        
        end_time = time.time()
        print(f"âœ… Perplexity æµå¼ç”ŸæˆæˆåŠŸ")
        print(f"ğŸ“Š ç»Ÿè®¡: {chunk_count} ä¸ªå—")
        print(f"â±ï¸ è€—æ—¶: {end_time - start_time:.2f}s")
        
    except Exception as e:
        print(f"âŒ Perplexity æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
    
    # æµ‹è¯•å…¶ä»–æä¾›å•†çš„æµå¼å¤„ç†ï¼ˆä»ä½¿ç”¨ requests æµï¼‰
    other_providers = [("groq", "llama-3.1-70b-versatile")]
    
    for provider, model in other_providers:
        print(f"\nğŸŒŠ æµ‹è¯•æµå¼ç”Ÿæˆ: {provider}")
        try:
            print(f"ğŸ“ æµå¼å“åº”: ", end="", flush=True)
            start_time = time.time()
            
            # è·å–æµå¯¹è±¡
            response = llm.stream_llm(
                provider=provider,
                model=model,
                messages=stream_test_messages,
                temperature=0.3,
                max_tokens=100
            )
            
            # å¤„ç† requests æµ
            chunk_count = 0
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        
                        try:
                            data = json.loads(data_str)
                            if 'choices' in data and data['choices']:
                                delta = data['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                if content:
                                    print(content, end='', flush=True)
                                    chunk_count += 1
                        except json.JSONDecodeError:
                            continue
            
            end_time = time.time()
            print()  # æ¢è¡Œ
            print(f"âœ… {provider} æµå¼ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“Š ç»Ÿè®¡: {chunk_count} ä¸ªå—")
            print(f"â±ï¸ è€—æ—¶: {end_time - start_time:.2f}s")
            
        except Exception as e:
            print(f"âŒ {provider} æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
    
    print(f"\n{'=' * 60}")
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼è¯·æ£€æŸ¥ä¸Šè¿°ç»“æœä»¥ç¡®è®¤å„æä¾›å•†çš„å·¥ä½œçŠ¶æ€ã€‚")
    print("ğŸŒŠ æµå¼ç”ŸæˆåŠŸèƒ½å·²æ·»åŠ ï¼Œæ”¯æŒå®æ—¶æ¥æ”¶å’Œå¤„ç†æ–‡æœ¬æµã€‚")