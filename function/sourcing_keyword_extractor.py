import json
import os
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from api.llm import LLMClient


class SourcingKeywordExtractor:
    """
    å¯»è®¿å…³é”®è¯æå–å™¨
    ç”¨äºä»å¯»è®¿ç­–ç•¥ç»“æœä¸­æå–ç”¨äºæœç´¢çš„å…³é”®è¯ï¼Œç»Ÿä¸€è¾“å‡ºä¸ºè‹±æ–‡æ ¼å¼
    """
    
    def __init__(
        self, 
        model: str = "gemini-2.5-flash-lite", 
        provider: str = "gemini",
        temperature: float = 0.1,  # ä½æ¸©åº¦ç¡®ä¿ç»“æ„åŒ–è¾“å‡ºç¨³å®š
        max_tokens: int = 2000,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0
    ):
        """
        åˆå§‹åŒ–å¯»è®¿å…³é”®è¯æå–å™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            provider: LLMæä¾›å•† ("openai", "gemini", "ali", "groq", "perplexity")
            temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤0.1ç¡®ä¿ç»“æ„åŒ–è¾“å‡ºç¨³å®š
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤2000
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)ï¼Œé»˜è®¤1.0
            frequency_penalty: é¢‘ç‡æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
            presence_penalty: å­˜åœ¨æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
        """
        self.llm_client = LLMClient()
        self.model = model
        self.provider = provider
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty
        self.presence_penalty = presence_penalty
        self.prompt_template = self._load_prompt_template()
    
    def _load_prompt_template(self) -> str:
        """
        ä»æ–‡ä»¶åŠ è½½promptæ¨¡æ¿
        
        Returns:
            promptæ¨¡æ¿å­—ç¬¦ä¸²
        """
        prompt_file = project_root / "prompt" / "sourcing_keyword_extractor_prompt.md"
        
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"Warning: Prompt file not found at {prompt_file}")
            return self._get_default_prompt()
        except Exception as e:
            print(f"Error loading prompt template: {e}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """
        è·å–é»˜è®¤çš„promptæ¨¡æ¿
        
        Returns:
            é»˜è®¤promptå­—ç¬¦ä¸²
        """
        return """You are a professional talent acquisition specialist. 
Extract searchable keywords from the provided sourcing plan result for talent hunting purposes.
Return the keywords in English JSON format: {"sourcing_keywords": ["keyword1", "keyword2", ...]}"""
    
    def _parse_json_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æJSONå“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        
        Args:
            content: å“åº”å†…å®¹
            
        Returns:
            è§£æåçš„JSONå­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        # æ¸…ç†å†…å®¹
        content = content.strip()
        
        # å°è¯•å¤šç§JSONè§£æç­–ç•¥
        json_patterns = [
            r'```json\s*(\{.*?\})\s*```',  # ```json {} ```
            r'```\s*(\{.*?\})\s*```',      # ``` {} ```
            r'(\{.*?\})',                  # ç›´æ¥çš„JSONå¯¹è±¡
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                try:
                    parsed = json.loads(match)
                    if isinstance(parsed, dict) and "sourcing_keywords" in parsed:
                        return parsed
                except json.JSONDecodeError:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´JSONï¼Œå°è¯•æå–å…³é”®è¯åˆ—è¡¨
        try:
            # æŸ¥æ‰¾ç±»ä¼¼ ["keyword1", "keyword2"] çš„æ¨¡å¼
            list_pattern = r'\[([^\]]*)\]'
            matches = re.findall(list_pattern, content)
            
            for match in matches:
                # æå–å¼•å·å†…çš„å†…å®¹
                keywords = re.findall(r'"([^"]*)"', match)
                if keywords:
                    return {"sourcing_keywords": keywords}
                    
                # å°è¯•é€—å·åˆ†å‰²
                keywords = [k.strip().strip('"\'') for k in match.split(',')]
                keywords = [k for k in keywords if k and len(k) > 1]
                if keywords:
                    return {"sourcing_keywords": keywords}
                    
        except Exception as e:
            print(f"Error in fallback parsing: {e}")
        
        return None
    
    def extract_sourcing_keywords(
        self, 
        sourcing_plan_content: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ä»å¯»è®¿ç­–ç•¥å†…å®¹ä¸­æå–å…³é”®è¯
        
        Args:
            sourcing_plan_content: å¯»è®¿ç­–ç•¥å†…å®¹
            **kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°
            
        Returns:
            åŒ…å«æå–ç»“æœçš„å­—å…¸
        """
        # éªŒè¯è¾“å…¥
        if not sourcing_plan_content or not sourcing_plan_content.strip():
            return {
                "success": False,
                "error": "Sourcing plan content is empty",
                "keywords": [],
                "json_result": None
            }
        
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = f"""{self.prompt_template}

## Sourcing Plan Content:

{sourcing_plan_content.strip()}

Please extract the most relevant keywords for talent searching and return them in the required JSON format."""
        
        messages = [
            {
                "role": "user",
                "content": full_prompt
            }
        ]
        
        # åˆå¹¶é»˜è®¤å‚æ•°å’Œä¼ å…¥å‚æ•°
        llm_params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
            **kwargs  # ä¼ å…¥çš„å‚æ•°ä¼šè¦†ç›–é»˜è®¤å‚æ•°
        }
        
        try:
            # è°ƒç”¨LLM
            response = self.llm_client.call_llm(
                provider=self.provider,
                model=self.model,
                messages=messages,
                **llm_params
            )
            
            if not response.get("success"):
                return {
                    "success": False,
                    "error": f"LLM call failed: {response.get('error', 'Unknown error')}",
                    "keywords": [],
                    "json_result": None,
                    "raw_response": response
                }
            
            # æå–å“åº”å†…å®¹
            content = self.llm_client.get_response_content(response)
            if not content:
                return {
                    "success": False,
                    "error": "No content in LLM response",
                    "keywords": [],
                    "json_result": None,
                    "raw_response": response
                }
            
            # è§£æJSONå“åº”
            parsed_result = self._parse_json_response(content)
            if not parsed_result:
                return {
                    "success": False,
                    "error": "Failed to parse JSON response",
                    "keywords": [],
                    "json_result": None,
                    "raw_content": content
                }
            
            # æå–å…³é”®è¯åˆ—è¡¨
            keywords = parsed_result.get("sourcing_keywords", [])
            if not isinstance(keywords, list):
                keywords = []
            
            # æ¸…ç†å’ŒéªŒè¯å…³é”®è¯
            clean_keywords = []
            for keyword in keywords:
                if isinstance(keyword, str) and len(keyword.strip()) > 0:
                    clean_keywords.append(keyword.strip())
            
            return {
                "success": True,
                "keywords": clean_keywords,
                "json_result": {"sourcing_keywords": clean_keywords},
                "raw_content": content,
                "usage": response.get("data", {}).get("usage", {}),
                "model_used": response.get("data", {}).get("model", self.model),
                "provider": self.provider
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Keyword extraction failed: {str(e)}",
                "keywords": [],
                "json_result": None
            }
    
    def save_keywords(
        self, 
        result: Dict[str, Any], 
        output_file: Optional[str] = None
    ) -> str:
        """
        ä¿å­˜å…³é”®è¯ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            result: å…³é”®è¯æå–ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not output_file:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"sourcing_keywords_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                if result.get("success"):
                    json.dump(result["json_result"], f, ensure_ascii=False, indent=2)
                else:
                    json.dump({
                        "success": False,
                        "error": result.get("error", "Unknown error"),
                        "sourcing_keywords": []
                    }, f, ensure_ascii=False, indent=2)
            
            return output_file
            
        except Exception as e:
            print(f"Failed to save keywords to file: {e}")
            return None
    
    def get_supported_providers(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„LLMæä¾›å•†åˆ—è¡¨
        
        Returns:
            æ”¯æŒçš„æä¾›å•†åˆ—è¡¨
        """
        return ["openai", "gemini", "ali", "groq", "perplexity"]
    
    def get_recommended_models(self, provider: str) -> List[str]:
        """
        è·å–æ¨èçš„æ¨¡å‹åˆ—è¡¨
        
        Args:
            provider: æä¾›å•†åç§°
            
        Returns:
            æ¨èçš„æ¨¡å‹åˆ—è¡¨
        """
        models = {
            "openai": ["gpt-4-1106-preview", "gpt-4", "gpt-3.5-turbo"],
            "gemini": ["gemini-2.5-flash-lite", "gemini-1.5-pro", "gemini-1.5-flash"],
            "ali": ["deepseek-v3", "qwen-max", "qwen-turbo", "qwen-plus"],
            "groq": ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"],
            "perplexity": ["llama-3.1-sonar-large-128k-online", "llama-3.1-sonar-small-128k-online"]
        }
        
        return models.get(provider.lower(), [])


if __name__ == "__main__":
    # ä¸­æ–‡å¯»è®¿ç­–ç•¥æµ‹è¯•æ¡ˆä¾‹
    sourcing_plan_cn = """
    # å²—ä½å®šä½è§£æ
    
    **æ ¸å¿ƒä»·å€¼**  
    è¯¥å²—ä½è´Ÿè´£è…¾è®¯ä¼šè®®äº§å“çš„æŒç»­ä¼˜åŒ–ä¸å•†ä¸šåŒ–ç­–ç•¥åˆ¶å®šï¼Œè§£å†³ç”¨æˆ·å¢é•¿ã€ä½“éªŒæå‡åŠå•†ä¸šå˜ç°çš„å…³é”®é—®é¢˜ã€‚
    
    # å¯»è®¿ç­–ç•¥è®¾è®¡
    
    ## ç²¾å‡†æ¸ é“
    
    | æ¸ é“ç±»å‹ | æ¸ é“ |
    |---------|------|
    | åŒä¸šæŒ–çŒ | é˜¿é‡Œäº‘ã€å­—èŠ‚è·³åŠ¨ã€åä¸ºäº‘ã€é’‰é’‰ã€Zoom |
    | ä¸“ä¸šç¤¾åŒº | çŸ¥ä¹ã€äººäººéƒ½æ˜¯äº§å“ç»ç†ã€PMCAFF |
    
    ## å…³é”®è¯çŸ©é˜µ
    
    | å¯»è®¿é˜¶æ®µ | å…³é”®è¯ç»„åˆç¤ºä¾‹ |
    |---------|---------------|
    | åˆæœŸå¹¿æ’’ç½‘ | åœ¨çº¿åŠå…¬ + äº§å“ç­–åˆ’ + ç”¨æˆ·å¢é•¿ + æ•°æ®åˆ†æ |
    | ä¸­æœŸç²¾å‡†æŒ–çŒ | ä¼ä¸šåä½œ + è§†é¢‘ä¼šè®® + å•†ä¸šåŒ– + ç”¨æˆ·è°ƒç ” |
    | åæœŸæ”»åš | 0-1 äº§å“æ­å»º + åƒä¸‡çº§ç”¨æˆ·è¿è¥ + å•†ä¸šæ¨¡å¼è®¾è®¡ |
    """
    
    # è‹±æ–‡å¯»è®¿ç­–ç•¥æµ‹è¯•æ¡ˆä¾‹
    sourcing_plan_en = """
    # Position Analysis
    
    **Core Value**  
    This role is responsible for backend system architecture and scalability optimization for social media platforms.
    
    # Sourcing Strategy Design
    
    ## Precise Channels
    
    | Channel Type | Specific Channels |
    |-------------|-------------------|
    | Industry Hunting | Meta, Google, Twitter, TikTok, Snapchat |
    | Professional Communities | GitHub, Stack Overflow, Reddit |
    
    ## Keyword Matrix
    
    | Sourcing Phase | Keyword Combinations |
    |---------------|---------------------|
    | Initial Broad Search | backend development + distributed systems + scalability + performance |
    | Mid-stage Precision | microservices + API design + database optimization + cloud architecture |
    | Final Focused Hunt | system design + high concurrency + real-time processing + technical leadership |
    """
    
    # åˆ›å»ºæå–å™¨å®ä¾‹
    extractor = SourcingKeywordExtractor(
        model="gemini-2.5-flash-lite",
        provider="gemini",
        temperature=0.1,
        max_tokens=2000
    )
    
    # æµ‹è¯•ä¸­æ–‡æ¡ˆä¾‹
    print("=" * 80)
    print("ğŸ‡¨ğŸ‡³ ä¸­æ–‡å¯»è®¿ç­–ç•¥å…³é”®è¯æå–æµ‹è¯•")
    print("=" * 80)
    print("ğŸ”„ Extracting keywords...")
    
    result_cn = extractor.extract_sourcing_keywords(sourcing_plan_cn)
    
    if result_cn["success"]:
        print("âœ… Extraction successful!")
        print(f"ğŸ“‹ Extracted Keywords ({len(result_cn['keywords'])} total):")
        for i, keyword in enumerate(result_cn['keywords'], 1):
            print(f"  {i}. {keyword}")
        print(f"\nğŸ”¤ JSON Result:")
        print(json.dumps(result_cn['json_result'], ensure_ascii=False, indent=2))
    else:
        print("âŒ Extraction failed!")
        print(f"Error: {result_cn['error']}")
    
    print("\n" + "=" * 80)
    print("ğŸ‡ºğŸ‡¸ è‹±æ–‡å¯»è®¿ç­–ç•¥å…³é”®è¯æå–æµ‹è¯•")
    print("=" * 80)
    print("ğŸ”„ Extracting keywords...")
    
    # æµ‹è¯•è‹±æ–‡æ¡ˆä¾‹
    result_en = extractor.extract_sourcing_keywords(sourcing_plan_en)
    
    if result_en["success"]:
        print("âœ… Extraction successful!")
        print(f"ğŸ“‹ Extracted Keywords ({len(result_en['keywords'])} total):")
        for i, keyword in enumerate(result_en['keywords'], 1):
            print(f"  {i}. {keyword}")
        print(f"\nğŸ”¤ JSON Result:")
        print(json.dumps(result_en['json_result'], ensure_ascii=False, indent=2))
    else:
        print("âŒ Extraction failed!")
        print(f"Error: {result_en['error']}")
    
    print("\n" + "=" * 80)
    print("ğŸ“Š Test Summary")
    print("=" * 80)
    print(f"ä¸­æ–‡æµ‹è¯•: {'âœ…' if result_cn['success'] else 'âŒ'}")
    print(f"è‹±æ–‡æµ‹è¯•: {'âœ…' if result_en['success'] else 'âŒ'}")
    
    # ä¿å­˜ç»“æœç¤ºä¾‹
    if result_cn["success"]:
        output_file = extractor.save_keywords(result_cn)
        if output_file:
            print(f"\nğŸ’¾ Keywords saved to: {output_file}")