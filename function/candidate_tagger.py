import json
import os
import sys
from typing import Dict, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from api.llm import LLMClient


class CandidateTagger:
    """
    å€™é€‰äººæ ‡ç­¾åˆ†æå™¨
    ç”¨äºåˆ†æèŒä½æè¿°æˆ–å€™é€‰äººæœç´¢æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«ç‰¹å®šçš„äº”ä¸ªç»´åº¦æ ‡å‡†
    """
    
    def __init__(
        self, 
        model: str = "gpt-4-1106-preview", 
        provider: str = "openai",
        temperature: float = 0.1,
        max_tokens: int = 500,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0
    ):
        """
        åˆå§‹åŒ–å€™é€‰äººæ ‡ç­¾å™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            provider: LLMæä¾›å•† ("openai" æˆ– "perplexity")
            temperature: æ¸©åº¦å‚æ•° (0-2)ï¼Œé»˜è®¤0.1ç¡®ä¿ä¸€è‡´æ€§
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤500
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)ï¼Œé»˜è®¤1.0
            frequency_penalty: é¢‘ç‡æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
            presence_penalty: å­˜åœ¨æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
        """
        self.llm_client = LLMClient()
        self.model = model
        self.provider = provider
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty
        self.presence_penalty = presence_penalty
        self.prompt_template = self._load_prompt_template()
    
    def _load_prompt_template(self) -> str:
        """
        ä»æ–‡ä»¶åŠ è½½promptæ¨¡æ¿
        
        Returns:
            promptæ¨¡æ¿å­—ç¬¦ä¸²
        """
        prompt_file = project_root / "prompt" / "candidate_tagger_prompt.md"
        
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›åŸºç¡€çš„prompt
            return self._get_default_prompt()
        except Exception as e:
            print(f"Error loading prompt template: {e}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """
        è·å–é»˜è®¤çš„promptæ¨¡æ¿
        
        Returns:
            é»˜è®¤promptå­—ç¬¦ä¸²
        """
        return """You are a professional candidate evaluation assistant. Analyze the given text and determine if it contains criteria for these 5 dimensions:

1. Location - Geographic requirements, office locations, remote work preferences
2. Job Title - Specific roles, positions, job functions, titles
3. Years of Experience - Experience requirements, seniority levels, years in field
4. Industry - Industry sectors, business domains, company types
5. Skills - Technical skills, soft skills, certifications, tools, technologies

Respond with valid JSON in this exact format:
{
    "result": [
        {"label": "Location", "containsCriteria": boolean},
        {"label": "Job Title", "containsCriteria": boolean},
        {"label": "Years of Experience", "containsCriteria": boolean},
        {"label": "Industry", "containsCriteria": boolean},
        {"label": "Skills", "containsCriteria": boolean}
    ]
}

Analyze this text:"""
    
    def analyze_text(self, text: str, **kwargs) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æœ¬å¹¶è¿”å›æ ‡ç­¾ç»“æœ
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬ï¼ˆèŒä½æè¿°æˆ–æœç´¢æŸ¥è¯¢ï¼‰
            **kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        if not text or not text.strip():
            return {
                "success": False,
                "error": "Input text is empty",
                "result": None
            }
        
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = f"{self.prompt_template}\n\n{text.strip()}"
        
        messages = [
            {
                "role": "user",
                "content": full_prompt
            }
        ]
        
        # åˆå¹¶é»˜è®¤å‚æ•°å’Œä¼ å…¥å‚æ•°
        llm_params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
            **kwargs  # ä¼ å…¥çš„å‚æ•°ä¼šè¦†ç›–é»˜è®¤å‚æ•°
        }
        
        try:
            # è°ƒç”¨LLM
            response = self.llm_client.call_llm(
                provider=self.provider,
                model=self.model,
                messages=messages,
                **llm_params
            )
            
            if not response.get("success"):
                return {
                    "success": False,
                    "error": f"LLM call failed: {response.get('error', 'Unknown error')}",
                    "result": None,
                    "raw_response": response
                }
            
            # æå–å“åº”å†…å®¹
            content = self.llm_client.get_response_content(response)
            if not content:
                return {
                    "success": False,
                    "error": "No content in LLM response",
                    "result": None,
                    "raw_response": response
                }
            
            # è§£æJSONå“åº”
            parsed_result = self._parse_llm_response(content)
            
            return {
                "success": True,
                "result": parsed_result,
                "raw_content": content,
                "usage": response.get("data", {}).get("usage", {}),
                "model_used": response.get("data", {}).get("model", self.model),
                "provider": self.provider
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Analysis failed: {str(e)}",
                "result": None
            }
    
    def _parse_llm_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æLLMè¿”å›çš„JSONå“åº”
        
        Args:
            content: LLMè¿”å›çš„åŸå§‹å†…å®¹
            
        Returns:
            è§£æåçš„ç»“æœå­—å…¸
        """
        # å°è¯•ç›´æ¥è§£æJSON
        try:
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content = content.strip()
            if content.startswith("```json"):
                content = content.replace("```json", "").replace("```", "").strip()
            elif content.startswith("```"):
                content = content.replace("```", "").strip()
            
            result = json.loads(content)
            
            # éªŒè¯å“åº”æ ¼å¼
            if self._validate_response_format(result):
                return result
            else:
                raise ValueError("Invalid response format")
                
        except json.JSONDecodeError as e:
            # JSONè§£æå¤±è´¥æ—¶çš„å¤„ç†
            print(f"JSON parsing failed: {e}")
            print(f"Content: {content}")
            return None
        except Exception as e:
            print(f"Response parsing failed: {e}")
            return None
    
    def _validate_response_format(self, result: Dict[str, Any]) -> bool:
        """
        éªŒè¯å“åº”æ ¼å¼æ˜¯å¦æ­£ç¡®
        
        Args:
            result: è§£æåçš„ç»“æœå­—å…¸
            
        Returns:
            æ˜¯å¦æ ¼å¼æ­£ç¡®
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰resulté”®
            if "result" not in result:
                return False
            
            result_list = result["result"]
            if not isinstance(result_list, list) or len(result_list) != 5:
                return False
            
            # æ£€æŸ¥æ¯ä¸ªé¡¹ç›®çš„æ ¼å¼
            expected_labels = ["Location", "Job Title", "Years of Experience", "Industry", "Skills"]
            
            for i, item in enumerate(result_list):
                if not isinstance(item, dict):
                    return False
                if "label" not in item or "containsCriteria" not in item:
                    return False
                if item["label"] != expected_labels[i]:
                    return False
                if not isinstance(item["containsCriteria"], bool):
                    return False
            
            return True
            
        except Exception:
            return False
    
    def batch_analyze(self, texts: list[str], **kwargs) -> list[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªæ–‡æœ¬
        
        Args:
            texts: è¦åˆ†æçš„æ–‡æœ¬åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        for i, text in enumerate(texts):
            print(f"Analyzing text {i+1}/{len(texts)}...")
            result = self.analyze_text(text, **kwargs)
            results.append(result)
        
        return results
    
    def get_summary_stats(self, results: list[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è·å–æ‰¹é‡åˆ†æç»“æœçš„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = len(results)
        successful = sum(1 for r in results if r.get("success"))
        failed = total - successful
        
        # ç»Ÿè®¡æ¯ä¸ªç»´åº¦çš„å‡ºç°é¢‘ç‡
        dimension_stats = {
            "Location": 0,
            "Job Title": 0, 
            "Years of Experience": 0,
            "Industry": 0,
            "Skills": 0
        }
        
        for result in results:
            if result.get("success") and result.get("result"):
                for item in result["result"]["result"]:
                    if item.get("containsCriteria"):
                        dimension_stats[item["label"]] += 1
        
        return {
            "total_analyzed": total,
            "successful": successful,
            "failed": failed,
            "success_rate": successful / total if total > 0 else 0,
            "dimension_frequencies": dimension_stats,
            "dimension_percentages": {
                k: v / successful * 100 if successful > 0 else 0 
                for k, v in dimension_stats.items()
            }
        }


if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("Usage: python candidate_tagger.py \"text to analyze\"")
        sys.exit(1)
    
    # è·å–è¾“å…¥æ–‡æœ¬
    text = sys.argv[1]
    
    # åˆ›å»ºæ ‡ç­¾å™¨å®ä¾‹ - ä½¿ç”¨nanoæ¨¡å‹ï¼Œtemperature=0
    # tagger = CandidateTagger(
    #     model="gpt-4.1-nano-2025-04-14",
    #     provider="openai", 
    #     temperature=0.0,
    #     max_tokens=500
    # )
    tagger = CandidateTagger(
        model="meta-llama/llama-4-scout-17b-16e-instruct",
        provider="groq", 
        temperature=0.0,
        max_tokens=500
    )
    
    
    # åˆ†ææ–‡æœ¬
    text="We are looking for a backend engineer for our financial department. We hope this candidate have more than 5 years Remote"
    print(f"Analyzing: {text}")
    print("-" * 50)
    
    result = tagger.analyze_text(text)
    
    # è¾“å‡ºç»“æœ
    if result["success"]:
        print("âœ… Analysis successful!")
        print("\nResult:")
        print(json.dumps(result["result"], indent=2, ensure_ascii=False))
        
        if "usage" in result:
            usage = result["usage"]
            print(f"\nğŸ“Š Usage Statistics:")
            print(f"  Prompt tokens: {usage.get('prompt_tokens', 'N/A')}")
            print(f"  Completion tokens: {usage.get('completion_tokens', 'N/A')}")
            print(f"  Total tokens: {usage.get('total_tokens', 'N/A')}")
        
        print(f"\nğŸ¤– Model: {result.get('model_used', 'N/A')}")
        print(f"ğŸ”§ Provider: {result.get('provider', 'N/A')}")
        
    else:
        print("âŒ Analysis failed!")
        print(f"Error: {result['error']}")
        if "raw_response" in result:
            print(f"Raw response: {result['raw_response']}")

