import json
import os
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from api.llm import LLMClient


class SourcingPlanGenerator:
    """
    å¯»è®¿ç­–ç•¥ç”Ÿæˆå™¨
    ç”¨äºåˆ†æèŒä½æè¿°ã€å…¬å¸åç§°å’Œå²—ä½æ ‡é¢˜ï¼Œç”Ÿæˆè¯¦ç»†çš„äººæ‰å¯»è®¿ç­–ç•¥å’Œå…³é”®è¯çŸ©é˜µ
    """
    
    def __init__(
        self, 
        model: str = "gemini-2.5-flash-lite", 
        provider: str = "gemini",
        temperature: float = 0.3,
        max_tokens: int = 4000,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0
    ):
        """
        åˆå§‹åŒ–å¯»è®¿ç­–ç•¥ç”Ÿæˆå™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            provider: LLMæä¾›å•† ("openai", "gemini", "ali", "groq", "perplexity")
            temperature: æ¸©åº¦å‚æ•° (0-2)ï¼Œé»˜è®¤0.3ç¡®ä¿åˆ›æ„æ€§åˆ†æ
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤4000æ”¯æŒé•¿ç¯‡åˆ†æ
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)ï¼Œé»˜è®¤1.0
            frequency_penalty: é¢‘ç‡æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
            presence_penalty: å­˜åœ¨æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
        """
        self.llm_client = LLMClient()
        self.model = model
        self.provider = provider
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty
        self.presence_penalty = presence_penalty
        self.prompt_template = self._load_prompt_template()
    
    def _load_prompt_template(self) -> str:
        """
        ä»æ–‡ä»¶åŠ è½½promptæ¨¡æ¿
        
        Returns:
            promptæ¨¡æ¿å­—ç¬¦ä¸²
        """
        prompt_file = project_root / "prompt" / "sourcing_plan_generator_prompt.md"
        
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"Warning: Prompt file not found at {prompt_file}")
            return self._get_default_prompt()
        except Exception as e:
            print(f"Error loading prompt template: {e}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """
        è·å–é»˜è®¤çš„promptæ¨¡æ¿
        
        Returns:
            é»˜è®¤promptå­—ç¬¦ä¸²
        """
        return """You are a professional talent acquisition specialist and recruiter. 
Analyze the provided job position and company information, then generate a comprehensive sourcing strategy 
with talent profiling, sourcing channels, and keyword matrices for effective talent hunting.

Please provide detailed sourcing plan covering:
1. Position Analysis and Value Assessment
2. Talent Profiling and Modeling
3. Sourcing Strategy Design
4. Keyword Matrix Development
5. Risk Control Checklist

Detect the input language and respond in the same language (Chinese or English)."""
    
    def _detect_language(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: è¦æ£€æµ‹çš„æ–‡æœ¬
            
        Returns:
            è¯­è¨€ç±»å‹ ("chinese" æˆ– "english")
        """
        # ç®€å•çš„ä¸­æ–‡å­—ç¬¦æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return "english"
        
        chinese_ratio = chinese_chars / total_chars
        return "chinese" if chinese_ratio > 0.3 else "english"
    
    def generate_sourcing_plan(
        self, 
        jd_content: str, 
        company_name: str, 
        position_title: str,
        output_language: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¯»è®¿ç­–ç•¥è®¡åˆ’
        
        Args:
            jd_content: èŒä½æè¿°å†…å®¹
            company_name: å…¬å¸åç§°
            position_title: å²—ä½æ ‡é¢˜
            output_language: è¾“å‡ºè¯­è¨€ ("auto", "chinese", "english")
            **kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°
            
        Returns:
            åŒ…å«å¯»è®¿ç­–ç•¥çš„å­—å…¸
        """
        # éªŒè¯è¾“å…¥
        if not jd_content or not jd_content.strip():
            return {
                "success": False,
                "error": "Job description content is empty",
                "result": None
            }
        
        if not company_name or not company_name.strip():
            return {
                "success": False,
                "error": "Company name is empty",
                "result": None
            }
        
        if not position_title or not position_title.strip():
            return {
                "success": False,
                "error": "Position title is empty", 
                "result": None
            }
        
        # ç¡®å®šè¾“å‡ºè¯­è¨€
        if output_language == "auto":
            combined_text = f"{jd_content} {company_name} {position_title}"
            detected_language = self._detect_language(combined_text)
        else:
            detected_language = output_language.lower()
        
        # æ ¹æ®è¯­è¨€è®¾ç½®æ·»åŠ è¯­è¨€æŒ‡å®šæŒ‡ä»¤
        language_instruction = ""
        if detected_language == "chinese":
            language_instruction = "\n\nIMPORTANT: Please respond in Chinese (ä¸­æ–‡)."
        elif detected_language == "english":
            language_instruction = "\n\nIMPORTANT: Please respond in English."
        
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = f"""{self.prompt_template}

## Input Information

**JD Content (èŒä½æè¿°):**
{jd_content.strip()}

**Company Name (å…¬å¸åç§°):**
{company_name.strip()}

**Position Title (å²—ä½æ ‡é¢˜):**
{position_title.strip()}

Please generate a comprehensive sourcing plan following the framework provided above.{language_instruction}"""
        
        messages = [
            {
                "role": "user",
                "content": full_prompt
            }
        ]
        
        # åˆå¹¶é»˜è®¤å‚æ•°å’Œä¼ å…¥å‚æ•°
        llm_params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
            **kwargs  # ä¼ å…¥çš„å‚æ•°ä¼šè¦†ç›–é»˜è®¤å‚æ•°
        }
        
        try:
            # è°ƒç”¨LLM
            response = self.llm_client.call_llm(
                provider=self.provider,
                model=self.model,
                messages=messages,
                **llm_params
            )
            
            if not response.get("success"):
                return {
                    "success": False,
                    "error": f"LLM call failed: {response.get('error', 'Unknown error')}",
                    "result": None,
                    "raw_response": response
                }
            
            # æå–å“åº”å†…å®¹
            content = self.llm_client.get_response_content(response)
            if not content:
                return {
                    "success": False,
                    "error": "No content in LLM response",
                    "result": None,
                    "raw_response": response
                }
            
            return {
                "success": True,
                "result": {
                    "sourcing_plan": content,
                    "input_info": {
                        "jd_content": jd_content,
                        "company_name": company_name,
                        "position_title": position_title
                    },
                    "detected_language": detected_language
                },
                "raw_content": content,
                "usage": response.get("data", {}).get("usage", {}),
                "model_used": response.get("data", {}).get("model", self.model),
                "provider": self.provider
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Sourcing plan generation failed: {str(e)}",
                "result": None
            }
    
    def save_sourcing_plan(
        self, 
        result: Dict[str, Any], 
        output_file: Optional[str] = None
    ) -> str:
        """
        ä¿å­˜å¯»è®¿ç­–ç•¥åˆ°æ–‡ä»¶
        
        Args:
            result: å¯»è®¿ç­–ç•¥ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not output_file:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            input_info = result["result"]["input_info"]
            company_name = input_info["company_name"].replace(" ", "_")
            position_title = input_info["position_title"].replace(" ", "_").replace("-", "_")
            output_file = f"sourcing_plan_{company_name}_{position_title}_{timestamp}.md"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                if result.get("success"):
                    # å†™å…¥å¯»è®¿ç­–ç•¥ç»“æœ
                    f.write(result["result"]["sourcing_plan"])
                else:
                    f.write("# SOURCING PLAN GENERATION FAILED\n\n")
                    f.write(f"Error: {result.get('error', 'Unknown error')}\n")
            
            return output_file
            
        except Exception as e:
            print(f"Failed to save sourcing plan to file: {e}")
            return None
    
    def get_supported_providers(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„LLMæä¾›å•†åˆ—è¡¨
        
        Returns:
            æ”¯æŒçš„æä¾›å•†åˆ—è¡¨
        """
        return ["openai", "gemini", "ali", "groq", "perplexity"]
    
    def get_recommended_models(self, provider: str) -> List[str]:
        """
        è·å–æ¨èçš„æ¨¡å‹åˆ—è¡¨
        
        Args:
            provider: æä¾›å•†åç§°
            
        Returns:
            æ¨èçš„æ¨¡å‹åˆ—è¡¨
        """
        models = {
            "openai": ["gpt-4-1106-preview", "gpt-4", "gpt-3.5-turbo"],
            "gemini": ["gemini-2.5-flash-lite", "gemini-1.5-pro", "gemini-1.5-flash"],
            "ali": ["deepseek-v3", "qwen-max", "qwen-turbo", "qwen-plus"],
            "groq": ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"],
            "perplexity": ["llama-3.1-sonar-large-128k-online", "llama-3.1-sonar-small-128k-online"]
        }
        
        return models.get(provider.lower(), [])


if __name__ == "__main__":
    # ä¸­æ–‡æµ‹è¯•æ¡ˆä¾‹
    jd_content_cn = """æˆ‘ä»¬æ­£åœ¨æ‹›è˜ä¸€åé«˜çº§äº§å“ç»ç†ï¼Œè´Ÿè´£è…¾è®¯ä¼šè®®äº§å“çš„è§„åˆ’å’Œä¼˜åŒ–ã€‚
    
    èŒè´£åŒ…æ‹¬ï¼š
    - è´Ÿè´£äº§å“åŠŸèƒ½è§„åˆ’ï¼Œä¸æŠ€æœ¯å›¢é˜Ÿåä½œæ¨è¿›äº§å“è¿­ä»£
    - åˆ†æç”¨æˆ·éœ€æ±‚å’Œå¸‚åœºè¶‹åŠ¿ï¼Œåˆ¶å®šäº§å“ç­–ç•¥
    - åè°ƒè·¨éƒ¨é—¨èµ„æºï¼Œæ¨åŠ¨äº§å“ç›®æ ‡è¾¾æˆ
    - ç›‘æ§äº§å“æ•°æ®ï¼ŒæŒç»­ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ
    
    è¦æ±‚ï¼š
    - 3-5å¹´äº§å“ç»ç†ç»éªŒï¼Œæœ‰Bç«¯æˆ–SaaSäº§å“ç»éªŒä¼˜å…ˆ
    - ç†Ÿæ‚‰æ•æ·å¼€å‘æµç¨‹ï¼Œæœ‰æŠ€æœ¯èƒŒæ™¯è€…ä¼˜å…ˆ
    - ä¼˜ç§€çš„æ•°æ®åˆ†æå’Œé€»è¾‘æ€ç»´èƒ½åŠ›
    - è‰¯å¥½çš„æ²Ÿé€šåè°ƒèƒ½åŠ›å’Œé¡¹ç›®ç®¡ç†èƒ½åŠ›
    """
    company_name_cn = "è…¾è®¯"
    position_title_cn = "é«˜çº§äº§å“ç»ç†"
    
    # è‹±æ–‡æµ‹è¯•æ¡ˆä¾‹
    jd_content_en = """We are seeking a Senior Software Engineer to join our engineering team at ByteDance.
    
    Responsibilities:
    - Design and develop scalable backend systems for our social media platform
    - Collaborate with cross-functional teams to deliver high-quality software solutions
    - Optimize system performance and ensure high availability
    - Mentor junior developers and contribute to technical documentation
    
    Requirements:
    - 5+ years of experience in backend development
    - Strong proficiency in Java, Python, or Go
    - Experience with distributed systems and microservices architecture
    - Excellent problem-solving skills and attention to detail
    """
    company_name_en = "ByteDance"
    position_title_en = "Senior Software Engineer"
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹ - ä½¿ç”¨Gemini 2.5
    generator = SourcingPlanGenerator(
        model="gemini-2.5-flash-lite",
        provider="gemini",
        temperature=0.7,  # æé«˜åˆ›é€ æ€§ï¼Œé¿å…å¡ä½
        max_tokens=12000  # å¢åŠ tokené™åˆ¶
    )
    
    # æµ‹è¯•ä¸­æ–‡æ¡ˆä¾‹
    print("=" * 80)
    print("ğŸ‡¨ğŸ‡³ ä¸­æ–‡æµ‹è¯•æ¡ˆä¾‹")
    print("=" * 80)
    print(f"ğŸ¢ Company: {company_name_cn}")
    print(f"ğŸ“‹ Position: {position_title_cn}")
    print("ğŸ”„ Generating sourcing plan...")
    
    result_cn = generator.generate_sourcing_plan(
        jd_content_cn, company_name_cn, position_title_cn, 
        output_language="chinese"
    )
    
    if result_cn["success"]:
        print("âœ… Generation successful!")
        print(result_cn["result"]["sourcing_plan"])
        
        # # ä¿å­˜ä¸­æ–‡ç»“æœ
        # output_file_cn = generator.save_sourcing_plan(result_cn)
        # if output_file_cn:
        #     print(f"\nğŸ’¾ Chinese result saved to: {output_file_cn}")
    else:
        print("âŒ Generation failed!")
        print(f"Error: {result_cn['error']}")
    
    print("\n" + "=" * 80)
    print("ğŸ‡ºğŸ‡¸ English Test Case")
    print("=" * 80)
    print(f"ğŸ¢ Company: {company_name_en}")
    print(f"ğŸ“‹ Position: {position_title_en}")
    print("ğŸ”„ Generating sourcing plan...")
    
    # æµ‹è¯•è‹±æ–‡æ¡ˆä¾‹
    result_en = generator.generate_sourcing_plan(
        jd_content_en, company_name_en, position_title_en, 
        output_language="english"
    )
    
    if result_en["success"]:
        print("âœ… Generation successful!")
        print(result_en["result"]["sourcing_plan"])
        
        # # ä¿å­˜è‹±æ–‡ç»“æœ
        # output_file_en = generator.save_sourcing_plan(result_en)
        # if output_file_en:
        #     print(f"\nğŸ’¾ English result saved to: {output_file_en}")
    else:
        print("âŒ Generation failed!")
        print(f"Error: {result_en['error']}")
    
    print("\n" + "=" * 80)
    print("ğŸ“Š Test Summary")
    print("=" * 80)
    print(f"ä¸­æ–‡æµ‹è¯•: {'âœ…' if result_cn['success'] else 'âŒ'}")
    print(f"è‹±æ–‡æµ‹è¯•: {'âœ…' if result_en['success'] else 'âŒ'}")