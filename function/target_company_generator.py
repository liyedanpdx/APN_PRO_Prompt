import json
import os
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from api.llm import LLMClient


class JobAnalyzer:
    """
    å²—ä½åˆ†æå™¨
    ç”¨äºåˆ†æèŒä½æè¿°ã€å…¬å¸åç§°å’Œå²—ä½æ ‡é¢˜ï¼Œæä¾›ä¸šåŠ¡æ¨¡å¼åˆ†æå’Œç›®æ ‡å…¬å¸æ¨è
    """
    
    def __init__(
        self, 
        model: str = "gpt-4-1106-preview", 
        provider: str = "openai",
        temperature: float = 0.3,
        max_tokens: int = 4000,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0
    ):
        """
        åˆå§‹åŒ–å²—ä½åˆ†æå™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            provider: LLMæä¾›å•† ("openai", "gemini", "ali", "groq", "perplexity")
            temperature: æ¸©åº¦å‚æ•° (0-2)ï¼Œé»˜è®¤0.3ç¡®ä¿åˆ›æ„æ€§åˆ†æ
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤4000æ”¯æŒé•¿ç¯‡åˆ†æ
            top_p: æ ¸é‡‡æ ·å‚æ•° (0-1)ï¼Œé»˜è®¤1.0
            frequency_penalty: é¢‘ç‡æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
            presence_penalty: å­˜åœ¨æƒ©ç½š (-2.0 to 2.0)ï¼Œé»˜è®¤0.0
        """
        self.llm_client = LLMClient()
        self.model = model
        self.provider = provider
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty
        self.presence_penalty = presence_penalty
        self.prompt_template = self._load_prompt_template()
    
    def _load_prompt_template(self) -> str:
        """
        ä»æ–‡ä»¶åŠ è½½promptæ¨¡æ¿
        
        Returns:
            promptæ¨¡æ¿å­—ç¬¦ä¸²
        """
        prompt_file = project_root / "prompt" / "target_company_generator_prompt.md"
        
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"Warning: Prompt file not found at {prompt_file}")
            return self._get_default_prompt()
        except Exception as e:
            print(f"Error loading prompt template: {e}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """
        è·å–é»˜è®¤çš„promptæ¨¡æ¿
        
        Returns:
            é»˜è®¤promptå­—ç¬¦ä¸²
        """
        return """You are a professional HR analyst and business intelligence expert. 
Analyze the provided job position and company information, then provide comprehensive business model analysis 
and target company recommendations for talent acquisition.

Please provide your analysis in a structured format covering:
1. Business Model Analysis
2. Position Analysis  
3. Target Company Recommendations

Detect the input language and respond in the same language (Chinese or English)."""
    
    def _detect_language(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: è¦æ£€æµ‹çš„æ–‡æœ¬
            
        Returns:
            è¯­è¨€ç±»å‹ ("chinese" æˆ– "english")
        """
        # ç®€å•çš„ä¸­æ–‡å­—ç¬¦æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return "english"
        
        chinese_ratio = chinese_chars / total_chars
        return "chinese" if chinese_ratio > 0.3 else "english"
    
    def analyze_job(
        self, 
        jd_content: str, 
        company_name: str, 
        position_title: str,
        output_language: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """
        åˆ†æå²—ä½å¹¶è¿”å›åˆ†æç»“æœ
        
        Args:
            jd_content: èŒä½æè¿°å†…å®¹
            company_name: å…¬å¸åç§°
            position_title: å²—ä½æ ‡é¢˜
            output_language: è¾“å‡ºè¯­è¨€ ("auto", "chinese", "english")
            **kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        # éªŒè¯è¾“å…¥
        if not jd_content or not jd_content.strip():
            return {
                "success": False,
                "error": "Job description content is empty",
                "result": None
            }
        
        if not company_name or not company_name.strip():
            return {
                "success": False,
                "error": "Company name is empty",
                "result": None
            }
        
        if not position_title or not position_title.strip():
            return {
                "success": False,
                "error": "Position title is empty", 
                "result": None
            }
        
        # ç¡®å®šè¾“å‡ºè¯­è¨€
        if output_language == "auto":
            combined_text = f"{jd_content} {company_name} {position_title}"
            detected_language = self._detect_language(combined_text)
        else:
            detected_language = output_language.lower()
        
        # æ ¹æ®è¯­è¨€è®¾ç½®æ·»åŠ è¯­è¨€æŒ‡å®šæŒ‡ä»¤
        language_instruction = ""
        if detected_language == "chinese":
            language_instruction = "\n\nIMPORTANT: Please respond in Chinese (ä¸­æ–‡)."
        elif detected_language == "english":
            language_instruction = "\n\nIMPORTANT: Please respond in English."
        
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = f"""{self.prompt_template}

## Input Information

**JD Content (èŒä½æè¿°):**
{jd_content.strip()}

**Company Name (å…¬å¸åç§°):**
{company_name.strip()}

**Position Title (å²—ä½æ ‡é¢˜):**
{position_title.strip()}

Please analyze this job position following the framework provided above.{language_instruction}"""
        
        messages = [
            {
                "role": "user",
                "content": full_prompt
            }
        ]
        
        # åˆå¹¶é»˜è®¤å‚æ•°å’Œä¼ å…¥å‚æ•°
        llm_params = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
            **kwargs  # ä¼ å…¥çš„å‚æ•°ä¼šè¦†ç›–é»˜è®¤å‚æ•°
        }
        
        try:
            # è°ƒç”¨LLM
            response = self.llm_client.call_llm(
                provider=self.provider,
                model=self.model,
                messages=messages,
                **llm_params
            )
            
            if not response.get("success"):
                return {
                    "success": False,
                    "error": f"LLM call failed: {response.get('error', 'Unknown error')}",
                    "result": None,
                    "raw_response": response
                }
            
            # æå–å“åº”å†…å®¹
            content = self.llm_client.get_response_content(response)
            if not content:
                return {
                    "success": False,
                    "error": "No content in LLM response",
                    "result": None,
                    "raw_response": response
                }
            
            return {
                "success": True,
                "result": {
                    "analysis": content,
                    "input_info": {
                        "jd_content": jd_content,
                        "company_name": company_name,
                        "position_title": position_title
                    },
                    "detected_language": detected_language
                },
                "raw_content": content,
                "usage": response.get("data", {}).get("usage", {}),
                "model_used": response.get("data", {}).get("model", self.model),
                "provider": self.provider
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Analysis failed: {str(e)}",
                "result": None
            }
    
    def save_analysis_result(
        self, 
        result: Dict[str, Any], 
        output_file: Optional[str] = None
    ) -> str:
        """
        ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            result: åˆ†æç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not output_file:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"job_analysis_result_{timestamp}.txt"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                if result.get("success"):
                    # å†™å…¥åˆ†æç»“æœ
                    f.write("=" * 80 + "\n")
                    f.write("JOB ANALYSIS RESULT\n")
                    f.write("=" * 80 + "\n\n")
                    
                    # è¾“å…¥ä¿¡æ¯
                    input_info = result["result"]["input_info"]
                    f.write("INPUT INFORMATION:\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"Company: {input_info['company_name']}\n")
                    f.write(f"Position: {input_info['position_title']}\n")
                    f.write(f"Language: {result['result']['detected_language']}\n")
                    f.write(f"Model: {result.get('model_used', 'N/A')}\n")
                    f.write(f"Provider: {result.get('provider', 'N/A')}\n\n")
                    
                    # JDå†…å®¹
                    f.write("JOB DESCRIPTION:\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{input_info['jd_content']}\n\n")
                    
                    # åˆ†æç»“æœ
                    f.write("ANALYSIS RESULT:\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{result['result']['analysis']}\n\n")
                    
                    # ä½¿ç”¨ç»Ÿè®¡
                    if result.get("usage"):
                        usage = result["usage"]
                        f.write("USAGE STATISTICS:\n")
                        f.write("-" * 40 + "\n")
                        f.write(f"Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\n")
                        f.write(f"Completion tokens: {usage.get('completion_tokens', 'N/A')}\n")
                        f.write(f"Total tokens: {usage.get('total_tokens', 'N/A')}\n")
                    
                else:
                    f.write("ANALYSIS FAILED\n")
                    f.write("=" * 40 + "\n")
                    f.write(f"Error: {result.get('error', 'Unknown error')}\n")
            
            return output_file
            
        except Exception as e:
            print(f"Failed to save result to file: {e}")
            return None
    
    def get_supported_providers(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„LLMæä¾›å•†åˆ—è¡¨
        
        Returns:
            æ”¯æŒçš„æä¾›å•†åˆ—è¡¨
        """
        return ["openai", "gemini", "ali", "groq", "perplexity"]
    
    def get_recommended_models(self, provider: str) -> List[str]:
        """
        è·å–æ¨èçš„æ¨¡å‹åˆ—è¡¨
        
        Args:
            provider: æä¾›å•†åç§°
            
        Returns:
            æ¨èçš„æ¨¡å‹åˆ—è¡¨
        """
        models = {
            "openai": ["gpt-4-1106-preview", "gpt-4", "gpt-3.5-turbo"],
            "gemini": ["gemini-2.5-flash-lite", "gemini-1.5-pro", "gemini-1.5-flash"],
            "ali": ["deepseek-v3", "qwen-max", "qwen-turbo", "qwen-plus"],
            "groq": ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"],
            "perplexity": ["llama-3.1-sonar-large-128k-online", "llama-3.1-sonar-small-128k-online"]
        }
        
        return models.get(provider.lower(), [])


if __name__ == "__main__":
    # ä¸­æ–‡æµ‹è¯•æ¡ˆä¾‹
    jd_content_cn = """æˆ‘ä»¬æ­£åœ¨æ‹›è˜ä¸€åé«˜çº§äº§å“ç»ç†ï¼Œè´Ÿè´£è…¾è®¯ä¼šè®®äº§å“çš„è§„åˆ’å’Œä¼˜åŒ–ã€‚
    
    èŒè´£åŒ…æ‹¬ï¼š
    - è´Ÿè´£äº§å“åŠŸèƒ½è§„åˆ’ï¼Œä¸æŠ€æœ¯å›¢é˜Ÿåä½œæ¨è¿›äº§å“è¿­ä»£
    - åˆ†æç”¨æˆ·éœ€æ±‚å’Œå¸‚åœºè¶‹åŠ¿ï¼Œåˆ¶å®šäº§å“ç­–ç•¥
    - åè°ƒè·¨éƒ¨é—¨èµ„æºï¼Œæ¨åŠ¨äº§å“ç›®æ ‡è¾¾æˆ
    - ç›‘æ§äº§å“æ•°æ®ï¼ŒæŒç»­ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ
    
    è¦æ±‚ï¼š
    - 3-5å¹´äº§å“ç»ç†ç»éªŒï¼Œæœ‰Bç«¯æˆ–SaaSäº§å“ç»éªŒä¼˜å…ˆ
    - ç†Ÿæ‚‰æ•æ·å¼€å‘æµç¨‹ï¼Œæœ‰æŠ€æœ¯èƒŒæ™¯è€…ä¼˜å…ˆ
    - ä¼˜ç§€çš„æ•°æ®åˆ†æå’Œé€»è¾‘æ€ç»´èƒ½åŠ›
    - è‰¯å¥½çš„æ²Ÿé€šåè°ƒèƒ½åŠ›å’Œé¡¹ç›®ç®¡ç†èƒ½åŠ›
    """
    company_name_cn = "è…¾è®¯"
    position_title_cn = "é«˜çº§äº§å“ç»ç†-è…¾è®¯ä¼šè®®äº§å“"
    
    # è‹±æ–‡æµ‹è¯•æ¡ˆä¾‹
    jd_content_en = """We are seeking a Senior Software Engineer to join our engineering team at ByteDance.
    
    Responsibilities:
    - Design and develop scalable backend systems for our social media platform
    - Collaborate with cross-functional teams to deliver high-quality software solutions
    - Optimize system performance and ensure high availability
    - Mentor junior developers and contribute to technical documentation
    
    Requirements:
    - 5+ years of experience in backend development
    - Strong proficiency in Java, Python, or Go
    - Experience with distributed systems and microservices architecture
    - Excellent problem-solving skills and attention to detail
    """
    company_name_en = "ByteDance"
    position_title_en = "Senior Software Engineer"
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹ - ä½¿ç”¨Gemini 2.5
    analyzer = JobAnalyzer(
        model="gemini-2.5-flash-lite",
        provider="gemini",
        temperature=0.3,
        max_tokens=10000
    )
    
    # æµ‹è¯•ä¸­æ–‡æ¡ˆä¾‹
    print("=" * 80)
    print("ğŸ‡¨ğŸ‡³ ä¸­æ–‡æµ‹è¯•æ¡ˆä¾‹")
    print("=" * 80)
    print(f"ğŸ¢ Company: {company_name_cn}")
    print(f"ğŸ“‹ Position: {position_title_cn}")
    print("ğŸ”„ Analyzing job position...")
    
    result_cn = analyzer.analyze_job(jd_content_cn, company_name_cn, position_title_cn, output_language="chinese")
    
    if result_cn["success"]:
        print("âœ… Analysis successful!")
        print(result_cn["result"]["analysis"])
    else:
        print("âŒ Analysis failed!")
        print(f"Error: {result_cn['error']}")
    
    print("\n" + "=" * 80)
    print("ğŸ‡ºğŸ‡¸ English Test Case")
    print("=" * 80)
    print(f"ğŸ¢ Company: {company_name_en}")
    print(f"ğŸ“‹ Position: {position_title_en}")
    print("ğŸ”„ Analyzing job position...")
    
    # æµ‹è¯•è‹±æ–‡æ¡ˆä¾‹
    analyzer = JobAnalyzer(
        model="gemini-2.5-flash-lite",
        provider="gemini",
        temperature=0.3,
        max_tokens=10000
    )
    
    result_en = analyzer.analyze_job(jd_content_en, company_name_en, position_title_en, output_language="english")
    
    if result_en["success"]:
        print("âœ… Analysis successful!")
        print(result_en["result"]["analysis"])
    else:
        print("âŒ Analysis failed!")
        print(f"Error: {result_en['error']}")
    
    print("\n" + "=" * 80)
    print("ğŸ“Š Test Summary")
    print("=" * 80)
    print(f"ä¸­æ–‡æµ‹è¯•: {'âœ…' if result_cn['success'] else 'âŒ'}")
    print(f"è‹±æ–‡æµ‹è¯•: {'âœ…' if result_en['success'] else 'âŒ'}")