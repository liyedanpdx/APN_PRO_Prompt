#!/usr/bin/env python3
"""
Groqæ¨¡å‹æµ‹è¯•ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•å¿«é€Ÿåˆ‡æ¢ä¸åŒçš„Groqæ¨¡å‹è¿›è¡Œæµ‹è¯•
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from test.candidate_tagger_test import CandidateTaggerPerformanceTest

# Groqå¯ç”¨æ¨¡å‹åˆ—è¡¨
GROQ_MODELS = {
    "llama-3.1-70b-versatile": "Llama 3.1 70B - æœ€å¼ºæ¨ç†èƒ½åŠ›",
    "llama-3.1-8b-instant": "Llama 3.1 8B - å¿«é€Ÿå“åº”",
    "llama-3.2-1b-preview": "Llama 3.2 1B - è½»é‡çº§æ¨¡å‹", 
    "llama-3.2-3b-preview": "Llama 3.2 3B - å¹³è¡¡æ€§èƒ½",
    "mixtral-8x7b-32768": "Mixtral 8x7B - é•¿ä¸Šä¸‹æ–‡",
    "gemma-7b-it": "Gemma 7B - Googleæ¨¡å‹",
    "gemma2-9b-it": "Gemma2 9B - æ”¹è¿›ç‰ˆæœ¬"
}

def quick_test_single_model(model_name: str, samples: int = 10):
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*50}")
    print(f"ğŸ§ª å¿«é€Ÿæµ‹è¯•: {model_name}")
    print(f"ğŸ“ æè¿°: {GROQ_MODELS.get(model_name, 'æœªçŸ¥æ¨¡å‹')}")
    print(f"{'='*50}")
    
    test_runner = CandidateTaggerPerformanceTest()
    
    results = test_runner.concurrent_test(
        total_samples=samples,
        concurrent_limit=5,  # è¾ƒå°çš„å¹¶å‘æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
        model=model_name,
        provider="groq",
        temperature=0.0
    )
    
    # ç®€åŒ–è¾“å‡º
    perf = results["performance_metrics"]
    tokens = results["token_usage"]
    
    print(f"\nğŸ“Š å¿«é€Ÿç»“æœ:")
    print(f"  æˆåŠŸç‡: {perf['success_rate']:.1f}%")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {perf['average_duration']:.3f}ç§’")
    print(f"  QPS: {perf['successful_requests_per_second']:.2f}")
    print(f"  å¹³å‡Token: {tokens['average_total_tokens']:.1f}")
    
    return results

def compare_groq_models():
    """æ¯”è¾ƒå¤šä¸ªGroqæ¨¡å‹çš„æ€§èƒ½"""
    print("ğŸš€ Groqæ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ¨¡å‹è¿›è¡Œå¯¹æ¯”
    test_models = [
        "llama-3.1-70b-versatile",  # æœ€å¼ºæ€§èƒ½
        "llama-3.1-8b-instant",    # å¿«é€Ÿå“åº”
        "mixtral-8x7b-32768"       # é•¿ä¸Šä¸‹æ–‡
    ]
    
    results_summary = []
    
    for model in test_models:
        result = quick_test_single_model(model, samples=20)
        
        perf = result["performance_metrics"]
        tokens = result["token_usage"]
        
        results_summary.append({
            "model": model,
            "success_rate": perf['success_rate'],
            "avg_duration": perf['average_duration'],
            "qps": perf['successful_requests_per_second'],
            "avg_tokens": tokens['average_total_tokens']
        })
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ“ˆ æ¨¡å‹å¯¹æ¯”æ€»ç»“")
    print(f"{'='*80}")
    print(f"{'æ¨¡å‹':<30} {'æˆåŠŸç‡':<8} {'å¹³å‡æ—¶é—´':<10} {'QPS':<8} {'å¹³å‡Token':<10}")
    print("-" * 80)
    
    for r in results_summary:
        print(f"{r['model']:<30} {r['success_rate']:<7.1f}% {r['avg_duration']:<9.3f}s {r['qps']:<7.2f} {r['avg_tokens']:<9.1f}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # å•æ¨¡å‹æµ‹è¯•
        model_name = sys.argv[1]
        if model_name in GROQ_MODELS:
            samples = int(sys.argv[2]) if len(sys.argv) > 2 else 10
            quick_test_single_model(model_name, samples)
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å‹: {model_name}")
            print("å¯ç”¨æ¨¡å‹:")
            for model, desc in GROQ_MODELS.items():
                print(f"  {model} - {desc}")
    else:
        # æ¨¡å‹å¯¹æ¯”æµ‹è¯•
        compare_groq_models()